[+++ b/docs/en/rest-api/ml/jobcounts.asciidoc, +==== Job Statistics, +  (string) For open jobs only, contains messages relating to the selection, +  of a node to run the job., +  (object) An object that describes the number of records processed and, +  any related error counts. See <<ml-datacounts,data counts objects>>., +  (string) A unique identifier for the job., +  (object) For open jobs only, contains information about the node where the, +  job runs. See <<ml-stats-node,node object>>., +  For example, `28746386s`., +  If the job had irrevocably failed, it must be force closed and then deleted., +  (string) A unique identifier for the job., +  (long) The number of records that are missing a field that the job is, +  configured to analyze. Records with missing fields are still processed because, +  it is possible that not all fields are missing. The value of, +  `processed_record_count` includes this count. +, +  (long) The number of records that are out of time sequence and, +  outside of the latency window. This information is applicable only when, +  you provide data to the job by using the <<ml-post-data,post data API>>., +  These out of order records are discarded, since jobs require time series data, +  to be in ascending chronological order., +  (long) The total number of fields in all the records that have been processed, +  by the job. Only fields that are specified in the detector configuration, +  object contribute to this count. The time stamp is not included in this count., +  This value includes records with missing fields, since they are nonetheless, +  analyzed. +, +  If you use data feeds and have aggregations in your search query,, +  the `processed_record_count` differs from the `input_record_count`. +, +  If you use the <<ml-post-data,post data API>> to provide data to the job,, +  the following records are not processed: +, +--, +--, +  (long) The number of buckets that contained few data points compared to the, +  expected number of data points. If your data contains many sparse buckets,, +  consider using a longer `bucket_span`., +  (long) The number of buckets for which new entities in incoming data were not, +  processed due to insufficient model memory. This situation is also signified, +  by a `hard_limit: memory_status` property value., +  (string) The status of the mathematical models., +  This property can have one of the following values:, +  `soft_limit`::: The models used more than 60% of the configured memory limit, +  and older unused models will be pruned to free up space., +  `hard_limit`::: The models used more space than the configured memory limit., +  As a result, not all incoming data was processed., +  (long) The number of bytes of memory used by the models. This is the maximum, +  value since the last time the model was persisted. If the job is closed,, +  this value indicates the latest size., +, +, +The `node` objects contains properties for the node that runs the job., +This information is available only for open jobs., +  (string) The unique identifier of the node., +  (string) The node name., +  (string) The host and port where transport HTTP connections are accepted., +  `max_running_jobs`::: The maximum number of concurrently open jobs that are, +  allowed per node., +++ b/docs/en/rest-api/ml/jobcounts.asciidoc, +==== Job Statistics, +  (string) For open jobs only, contains messages relating to the selection, +  of a node to run the job., +  (object) An object that describes the number of records processed and, +  any related error counts. See <<ml-datacounts,data counts objects>>., +  (string) A unique identifier for the job., +  (object) For open jobs only, contains information about the node where the, +  job runs. See <<ml-stats-node,node object>>., +  For example, `28746386s`., +  If the job had irrevocably failed, it must be force closed and then deleted., +  (string) A unique identifier for the job., +  (long) The number of records that are missing a field that the job is, +  configured to analyze. Records with missing fields are still processed because, +  it is possible that not all fields are missing. The value of, +  `processed_record_count` includes this count. +, +  (long) The number of records that are out of time sequence and, +  outside of the latency window. This information is applicable only when, +  you provide data to the job by using the <<ml-post-data,post data API>>., +  These out of order records are discarded, since jobs require time series data, +  to be in ascending chronological order., +  (long) The total number of fields in all the records that have been processed, +  by the job. Only fields that are specified in the detector configuration, +  object contribute to this count. The time stamp is not included in this count., +  This value includes records with missing fields, since they are nonetheless, +  analyzed. +, +  If you use data feeds and have aggregations in your search query,, +  the `processed_record_count` differs from the `input_record_count`. +, +  If you use the <<ml-post-data,post data API>> to provide data to the job,, +  the following records are not processed: +, +--, +--, +  (long) The number of buckets that contained few data points compared to the, +  expected number of data points. If your data contains many sparse buckets,, +  consider using a longer `bucket_span`., +  (long) The number of buckets for which new entities in incoming data were not, +  processed due to insufficient model memory. This situation is also signified, +  by a `hard_limit: memory_status` property value., +  (string) The status of the mathematical models., +  This property can have one of the following values:, +  `soft_limit`::: The models used more than 60% of the configured memory limit, +  and older unused models will be pruned to free up space., +  `hard_limit`::: The models used more space than the configured memory limit., +  As a result, not all incoming data was processed.]