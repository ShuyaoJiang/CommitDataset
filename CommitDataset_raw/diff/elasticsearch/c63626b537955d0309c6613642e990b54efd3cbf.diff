[+++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java, +import org.apache.lucene.util.BytesRef;, +                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should, +                    try {, +                        // in 1.4.0 we added additional hashes for .si / segments_N files, +                        // to ensure we don't double the space in the repo since old snapshots, +                        // don't have this hash we try to read that hash from the blob store, +                        // in a bwc compatible way., +                        maybeRecalculateMetadataHash(blobContainer, fileInfo, metadata);, +                    }  catch (Throwable e) {, +                        logger.warn("{} Can't calculate hash from blob for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());, +                    }, +     * This is a BWC layer to ensure we update the snapshots metdata with the corresponding hashes before we compare them., +     * The new logic for StoreFileMetaData reads the entire <tt>.si</tt> and <tt>segments.n</tt> files to strengthen the, +     * comparison of the files on a per-segment / per-commit level., +     */, +    private static final void maybeRecalculateMetadataHash(ImmutableBlobContainer blobContainer, FileInfo fileInfo, Store.MetadataSnapshot snapshot) throws IOException {, +        final StoreFileMetaData metadata;, +        if (fileInfo != null && (metadata = snapshot.get(fileInfo.name())) != null) {, +            if (metadata.hash().length > 0 && fileInfo.metadata().hash().length == 0) {, +                // we have a hash - check if our repo has a hash too otherwise we have, +                // to calculate it., +                byte[] bytes = blobContainer.readBlobFully(fileInfo.physicalName());, +                final BytesRef spare = new BytesRef(bytes);, +                Store.MetadataSnapshot.hashFile(fileInfo.metadata().hash(), spare);, +            }, +        }, +, +    }, +, +    /**, +                    try {, +                        // in 1.4.0 we added additional hashes for .si / segments_N files, +                        // to ensure we don't double the space in the repo since old snapshots, +                        // don't have this hash we try to read that hash from the blob store, +                        // in a bwc compatible way., +                        maybeRecalculateMetadataHash(blobContainer, fileInfo, recoveryTargetMetadata);, +                    }  catch (Throwable e) {, +                        // if the index is broken we might not be able to read it, +                        logger.warn("{} Can't calculate hash from blog for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());, +                    }, +++ b/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java, +import org.apache.lucene.util.BytesRef;, +                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should, +                    try {, +                        // in 1.4.0 we added additional hashes for .si / segments_N files, +                        // to ensure we don't double the space in the repo since old snapshots, +                        // don't have this hash we try to read that hash from the blob store, +                        // in a bwc compatible way., +                        maybeRecalculateMetadataHash(blobContainer, fileInfo, metadata);, +                    }  catch (Throwable e) {, +                        logger.warn("{} Can't calculate hash from blob for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());, +                    }, +     * This is a BWC layer to ensure we update the snapshots metdata with the corresponding hashes before we compare them., +     * The new logic for StoreFileMetaData reads the entire <tt>.si</tt> and <tt>segments.n</tt> files to strengthen the, +     * comparison of the files on a per-segment / per-commit level., +     */, +    private static final void maybeRecalculateMetadataHash(ImmutableBlobContainer blobContainer, FileInfo fileInfo, Store.MetadataSnapshot snapshot) throws IOException {, +        final StoreFileMetaData metadata;, +        if (fileInfo != null && (metadata = snapshot.get(fileInfo.name())) != null) {, +            if (metadata.hash().length > 0 && fileInfo.metadata().hash().length == 0) {, +                // we have a hash - check if our repo has a hash too otherwise we have, +                // to calculate it., +                byte[] bytes = blobContainer.readBlobFully(fileInfo.physicalName());, +                final BytesRef spare = new BytesRef(bytes);, +                Store.MetadataSnapshot.hashFile(fileInfo.metadata().hash(), spare);, +            }, +        }, +, +    }, +, +    /**, +                    try {, +                        // in 1.4.0 we added additional hashes for .si / segments_N files, +                        // to ensure we don't double the space in the repo since old snapshots, +                        // don't have this hash we try to read that hash from the blob store, +                        // in a bwc compatible way., +                        maybeRecalculateMetadataHash(blobContainer, fileInfo, recoveryTargetMetadata);, +                    }  catch (Throwable e) {, +                        // if the index is broken we might not be able to read it, +                        logger.warn("{} Can't calculate hash from blog for file [{}] [{}]", e, shardId, fileInfo.physicalName(), fileInfo.metadata());, +                    }, +++ b/src/main/java/org/elasticsearch/index/store/Store.java, +            final BytesRef fileHash = new BytesRef();, +                       hashFile(fileHash, in);, +        /**, +         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB, +         */, +        public static void hashFile(BytesRef fileHash, IndexInput in) throws IOException {, +            final int len = (int)Math.min(1024 * 1024, in.length()); // for safety we limit this to 1MB, +            fileHash.offset = 0;, +            fileHash.grow(len);, +            fileHash.length = len;, +            in.readBytes(fileHash.bytes, 0, len);, +        }, +, +        /**, +         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB, +         */, +        public static void hashFile(BytesRef fileHash, BytesRef source) throws IOException {]