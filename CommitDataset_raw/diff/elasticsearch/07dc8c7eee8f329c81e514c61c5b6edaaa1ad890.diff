[+++ b/server/src/test/java/org/elasticsearch/indices/state/CloseWhileRelocatingShardsIT.java, +import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteRequest;, +import org.elasticsearch.cluster.node.DiscoveryNode;, +import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;, +import org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider;, +import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.Rebalance;, +import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;, +import org.elasticsearch.cluster.service.ClusterService;, +import org.elasticsearch.indices.recovery.PeerRecoverySourceService;, +import org.elasticsearch.indices.recovery.StartRecoveryRequest;, +import org.elasticsearch.plugins.Plugin;, +import org.elasticsearch.test.junit.annotations.TestLogging;, +import org.elasticsearch.test.transport.MockTransportService;, +import org.elasticsearch.transport.TransportService;, +import java.util.Collection;, +import java.util.stream.Collectors;, +import java.util.stream.IntStream;, +import static java.util.Collections.singletonList;, +    protected Collection<Class<? extends Plugin>> nodePlugins() {, +        return singletonList(MockTransportService.TestPlugin.class);, +    }, +, +    @Override, +            .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), Integer.MAX_VALUE), +            .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.getKey(), -1), +    protected int maximumNumberOfShards() {, +        return 3;, +    @TestLogging("org.elasticsearch.cluster.metadata.MetaDataIndexStateService:DEBUG,org.elasticsearch.action.admin.indices.close:DEBUG"), +        final String[] indices = new String[randomIntBetween(3, 5)];, +        final Map<String, BackgroundIndexer> indexers = new HashMap<>();, +            switch (i) {, +                case 0:, +                    logger.debug("creating empty index {}", indexName);, +                    createIndex(indexName);, +                    break;, +                case 1:, +                    nbDocs = scaledRandomIntBetween(1, 100);, +                    logger.debug("creating index {} with {} documents", indexName, nbDocs);, +                    createIndex(indexName);, +                    indexRandom(randomBoolean(), IntStream.range(0, nbDocs), +                        .mapToObj(n -> client().prepareIndex(indexName, "_doc").setSource("num", n)), +                        .collect(Collectors.toList()));, +                    break;, +                default:, +                    logger.debug("creating index {} with background indexing", indexName);, +                    final BackgroundIndexer indexer = new BackgroundIndexer(indexName, "_doc", client(), -1, 1);, +                    indexers.put(indexName, indexer);, +                    waitForDocs(1, indexer);, +                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), Rebalance.NONE.toString())));, +        final String targetNode = internalCluster().startDataOnlyNode();, +        ensureClusterSizeConsistency(); // wait for the master to finish processing join., +            final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, internalCluster().getMasterName());, +            final CountDownLatch latch = new CountDownLatch(indices.length);, +            final CountDownLatch release = new CountDownLatch(1);, +            // relocate one shard for every index to be closed, +            final AllocationCommands commands = new AllocationCommands();, +            for (final String index : indices) {, +                final NumShards numShards = getNumShards(index);, +                final int shardId = numShards.numPrimaries == 1 ? 0 : randomIntBetween(0, numShards.numPrimaries - 1);, +                final IndexRoutingTable indexRoutingTable = clusterService.state().routingTable().index(index);, +, +                final ShardRouting primary = indexRoutingTable.shard(shardId).primaryShard();, +, +                String currentNodeId = primary.currentNodeId();, +                if (numShards.numReplicas > 0) {, +                    final ShardRouting replica = indexRoutingTable.shard(shardId).replicaShards().iterator().next();, +                    if (randomBoolean()) {, +                        currentNodeId = replica.currentNodeId();, +                    }, +                }, +                final DiscoveryNode sourceNode = clusterService.state().nodes().resolveNode(primary.currentNodeId());, +                ((MockTransportService) internalCluster().getInstance(TransportService.class, targetNode)), +                    .addSendBehavior(internalCluster().getInstance(TransportService.class, sourceNode.getName()),, +                        (connection, requestId, action, request, options) -> {, +                            if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action)) {, +                                logger.debug("blocking recovery of shard {}", ((StartRecoveryRequest) request).shardId());, +                                latch.countDown();, +                                    release.await();, +                                    logger.debug("releasing recovery of shard {}", ((StartRecoveryRequest) request).shardId());, +                            connection.sendRequest(requestId, action, request, options);, +                        }, +                    );, +                commands.add(new MoveAllocationCommand(index, shardId, currentNodeId, targetNode));, +            assertAcked(client().admin().cluster().reroute(new ClusterRerouteRequest().commands(commands)).get());, +, +            final List<Thread> threads = new ArrayList<>();, +                    } finally {, +                        release.countDown();, +                    // Closing is not always acknowledged when shards are relocating: this is the case when the target shard is initializing, +                    // or is catching up operations. In these cases the TransportVerifyShardBeforeCloseAction will detect that the global, +                    // and max sequence number don't match and will not ack the close., +, +                .setTransientSettings(Settings.builder(), +                    .putNull(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey())));]