[+++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/GetRecordsAction.java, +            jobProvider.records(request.jobId, query, page -> listener.onResponse(new Response(page)), listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/GetRecordsAction.java, +            jobProvider.records(request.jobId, query, page -> listener.onResponse(new Response(page)), listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/UpdateModelSnapshotAction.java, +                    jobManager.updateModelSnapshot(modelSnapshot, b -> {, +            }, listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/GetRecordsAction.java, +            jobProvider.records(request.jobId, query, page -> listener.onResponse(new Response(page)), listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/UpdateModelSnapshotAction.java, +                    jobManager.updateModelSnapshot(modelSnapshot, b -> {, +            }, listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/job/manager/JobManager.java, +import org.elasticsearch.xpack.ml.job.persistence.AnomalyDetectorsIndex;, +    public void updateModelSnapshot(ModelSnapshot modelSnapshot, Consumer<Boolean> handler, Consumer<Exception> errorHandler) {, +        jobResultsPersister.updateModelSnapshot(modelSnapshot, handler, errorHandler);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/GetRecordsAction.java, +            jobProvider.records(request.jobId, query, page -> listener.onResponse(new Response(page)), listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/action/UpdateModelSnapshotAction.java, +                    jobManager.updateModelSnapshot(modelSnapshot, b -> {, +            }, listener::onFailure);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/job/manager/JobManager.java, +import org.elasticsearch.xpack.ml.job.persistence.AnomalyDetectorsIndex;, +    public void updateModelSnapshot(ModelSnapshot modelSnapshot, Consumer<Boolean> handler, Consumer<Exception> errorHandler) {, +        jobResultsPersister.updateModelSnapshot(modelSnapshot, handler, errorHandler);, +++ b/elasticsearch/src/main/java/org/elasticsearch/xpack/ml/job/persistence/JobProvider.java, +import java.util.concurrent.CountDownLatch;, +import java.util.concurrent.atomic.AtomicReference;, +                if (query.isExpand()) {, +                    Iterator<Bucket> bucketsToExpand = buckets.results().stream(), +                            .filter(bucket -> bucket.getRecordCount() > 0).iterator();, +                    expandBuckets(jobId, query, buckets, bucketsToExpand, 0, handler, errorHandler);, +                    return;, +                if (query.isExpand()) {, +                    Iterator<Bucket> bucketsToExpand = buckets.results().stream(), +                            .filter(bucket -> bucket.getRecordCount() > 0).iterator();, +                    expandBuckets(jobId, query, buckets, bucketsToExpand, 0, handler, errorHandler);, +                    return;, +    private void expandBuckets(String jobId, BucketsQuery query, QueryPage<Bucket> buckets, Iterator<Bucket> bucketsToExpand,, +                               int from, Consumer<QueryPage<Bucket>> handler, Consumer<Exception> errorHandler) {, +        if (bucketsToExpand.hasNext()) {, +            Consumer<Integer> c = i -> {, +                expandBuckets(jobId, query, buckets, bucketsToExpand, from + RECORDS_SIZE_PARAM, handler, errorHandler);, +            };, +            expandBucket(jobId, query.isIncludeInterim(), bucketsToExpand.next(), query.getPartitionValue(), from, c, errorHandler);, +        } else {, +            handler.accept(buckets);, +        }, +    }, +, +    public void expandBucket(String jobId, boolean includeInterim, Bucket bucket, String partitionFieldValue, int from,, +                             Consumer<Integer> consumer, Consumer<Exception> errorHandler) {, +        Consumer<QueryPage<AnomalyRecord>> h = page -> {, +            if (partitionFieldValue != null) {, +                bucket.setAnomalyScore(bucket.partitionAnomalyScore(partitionFieldValue));, +            }, +            if (page.count() > from + RECORDS_SIZE_PARAM) {, +                expandBucket(jobId, includeInterim, bucket, partitionFieldValue, from + RECORDS_SIZE_PARAM, consumer, errorHandler);, +            } else {, +                consumer.accept(bucket.getRecords().size());, +            }, +        };, +        bucketRecords(jobId, bucket, from, RECORDS_SIZE_PARAM, includeInterim, AnomalyRecord.PROBABILITY.getPreferredName(),, +                false, partitionFieldValue, h, errorHandler);, +    // keep blocking variant around for ScoresUpdater as that can remain a blocking as this is ran from dedicated ml threadpool., +    // also refactoring that to be non blocking is a lot of work., +    public int expandBucket(String jobId, boolean includeInterim, Bucket bucket) {, +        CountDownLatch latch = new CountDownLatch(1);, +        AtomicReference<Integer> holder = new AtomicReference<>();, +        AtomicReference<Exception> errorHolder = new AtomicReference<>();, +        expandBucket(jobId, includeInterim, bucket, null, 0, records -> {, +            holder.set(records);, +            latch.countDown();, +        }, e -> {, +            errorHolder.set(e);, +            latch.countDown();, +        });, +        try {, +            latch.await();, +        } catch (InterruptedException e) {, +            Thread.currentThread().interrupt();, +        }, +        if (errorHolder.get() != null) {, +            throw new RuntimeException(errorHolder.get());, +        } else {, +            return holder.get();, +        }, +    void bucketRecords(String jobId, Bucket bucket, int from, int size, boolean includeInterim, String sortField,, +                       boolean descending, String partitionFieldValue, Consumer<QueryPage<AnomalyRecord>> handler,, +                       Consumer<Exception> errorHandler) {, +        ResultsFilterBuilder builder = new ResultsFilterBuilder(recordFilter), +                .interim(AnomalyRecord.IS_INTERIM.getPreferredName(), includeInterim);, +        if (partitionFieldValue != null) {, +            builder.term(AnomalyRecord.PARTITION_FIELD_VALUE.getPreferredName(), partitionFieldValue);, +        }, +        recordFilter = builder.build();, +        records(jobId, from, size, recordFilter, sb, SECONDARY_SORT, descending, handler, errorHandler);, +    public void records(String jobId, RecordsQueryBuilder.RecordsQuery query, Consumer<QueryPage<AnomalyRecord>> handler,, +                        Consumer<Exception> errorHandler) {, +        records(jobId, query.getFrom(), query.getSize(), fb, sb, SECONDARY_SORT, query.isSortDescending(), handler, errorHandler);]