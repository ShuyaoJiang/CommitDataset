[+++ b/docs/reference/search/rank-eval.asciidoc, +=== Ranking evaluation request structure, +=== Template based ranking evaluation, +=== Available evaluation metrics, +==== Precision at k (Prec@k), +==== Mean reciprocal rank, +==== Discounted cumulative gain (DCG), +=== Response format, +The response of the `_rank_eval` endpoint contains the overall calculated result for the defined quality metric, , +a `details` section with a breakdown of results for each query in the test suite and an optional `failures` section, +that shows potential errors of individual queries. The response has the following format:, +[source,js], +--------------------------------, +{, +    "rank_eval": {, +        "quality_level": 0.4, <1>, +        "details": {  , +            "my_query_id1": { <2>, +                "quality_level": 0.6, <3>, +                "unknown_docs": [ <4>, +                    {, +                        "_index": "my_index",, +                        "_id": "1960795", +                    }, [...], +                ],, +                "hits": [, +                    {, +                        "hit": { <5>, +                            "_index": "my_index",, +                            "_type": "page",, +                            "_id": "1528558",, +                            "_score": 7.0556192, +                        },, +                        "rating": 1, +                    }, [...], +                ],, +                "metric_details": { <6>, +                    "relevant_docs_retrieved": 6,, +                    "docs_retrieved": 10, +                }, +            },, +            "my_query_id2 : { [...]} , +        },, +        "failures": { [...] }, +    }, +}, +--------------------------------, +// NOTCONSOLE, +, +<1> the overall evaluation quality calculated by the defined metric, +<2> the `details` section contains one entry for every query in the original `requests` section, keyed by the search request id, +<3> the `quality_level` in the `details` section shows the contribution of this query to the global quality score, +<4> the `unknown_docs` section contains an `_index` and `_id` entry for each document in the search result for this, +query that didn't have a ratings value. This can be used to ask the user to supply ratings for these documents, +<5> the `hits` section shows a grouping of the search results with their supplied rating, +<6> the `metric_details` give additional information about the calculated quality metric (e.g. how many of the retrieved, +documents where relevant). The content varies for each metric but allows for better interpretation of the results, +++ b/docs/reference/search/rank-eval.asciidoc, +=== Ranking evaluation request structure, +=== Template based ranking evaluation, +=== Available evaluation metrics, +==== Precision at k (Prec@k), +==== Mean reciprocal rank, +==== Discounted cumulative gain (DCG), +=== Response format, +The response of the `_rank_eval` endpoint contains the overall calculated result for the defined quality metric, , +a `details` section with a breakdown of results for each query in the test suite and an optional `failures` section, +that shows potential errors of individual queries. The response has the following format:, +[source,js], +--------------------------------, +{, +    "rank_eval": {, +        "quality_level": 0.4, <1>, +        "details": {  , +            "my_query_id1": { <2>, +                "quality_level": 0.6, <3>, +                "unknown_docs": [ <4>, +                    {, +                        "_index": "my_index",, +                        "_id": "1960795", +                    }, [...], +                ],, +                "hits": [, +                    {, +                        "hit": { <5>, +                            "_index": "my_index",, +                            "_type": "page",, +                            "_id": "1528558",, +                            "_score": 7.0556192, +                        },, +                        "rating": 1, +                    }, [...], +                ],, +                "metric_details": { <6>, +                    "relevant_docs_retrieved": 6,, +                    "docs_retrieved": 10, +                }, +            },, +            "my_query_id2 : { [...]} , +        },]