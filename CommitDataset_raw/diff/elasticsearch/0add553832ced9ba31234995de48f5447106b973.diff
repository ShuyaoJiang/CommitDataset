[+++ b/.gitignore, +/.project, +++ b/.gitignore, +/.project, +++ b/CONTRIBUTING.md, +**Repository:** [https://github.com/elasticsearch/elasticsearch-analysis-kuromoji](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji), +cd elasticsearch-analysis-kuromoji/, +++ b/.gitignore, +/.project, +++ b/CONTRIBUTING.md, +**Repository:** [https://github.com/elasticsearch/elasticsearch-analysis-kuromoji](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji), +cd elasticsearch-analysis-kuromoji/, +++ b/README.md, +Japanese (kuromoji) Analysis for Elasticsearch, +The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis module into elasticsearch., +In order to install the plugin, simply run: `bin/plugin -install elasticsearch/elasticsearch-analysis-kuromoji/2.0.0`., +* For master elasticsearch versions, look at [master branch](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/master)., +* For 1.2.x elasticsearch versions, look at [es-1.2 branch](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/es-1.2)., +* For 1.1.x elasticsearch versions, look at [es-1.1 branch](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/es-1.1)., +* For 1.0.x elasticsearch versions, look at [es-1.0 branch](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/es-1.0)., +* For 0.90.x elasticsearch versions, look at [es-0.90 branch](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/es-0.90)., +, +|  Kuromoji Analysis Plugin   |    elasticsearch    | Release date |, +|-----------------------------|---------------------|:------------:|, +| 3.0.0-SNAPSHOT              | master (2.x)        |  XXXX-XX-XX  |, +* [3.0.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/blob/master/README.md), +, +The plugin includes the `kuromoji` analyzer., +, +Includes Analyzer, Tokenizer, TokenFilter, +----------------------------------------, +, +The plugin includes these analyzer and tokenizer, tokenfilter., +, +| name                    | type        |, +|-------------------------|-------------|, +| kuromoji_iteration_mark | charfilter  |, +| kuromoji                | analyzer    |, +| kuromoji_tokenizer      | tokenizer   |, +| kuromoji_baseform       | tokenfilter |, +| kuromoji_part_of_speech | tokenfilter |, +| kuromoji_readingform    | tokenfilter |, +| kuromoji_stemmer        | tokenfilter |, +, +, +Usage, +-----, +, +## Analyzer : kuromoji, +, +An analyzer of type `kuromoji`., +This analyzer is the following tokenizer and tokenfilter combination., +, +* `kuromoji_tokenizer` : Kuromoji Tokenizer, +* `kuromoji_baseform` : Kuromoji BasicFormFilter (TokenFilter), +* `kuromoji_part_of_speech` : Kuromoji Part of Speech Stop Filter (TokenFilter), +* `cjk_width` : CJK Width Filter (TokenFilter), +* `stop` : Stop Filter (TokenFilter), +* `kuromoji_stemmer` : Kuromoji Katakana Stemmer Filter(TokenFilter), +* `lowercase` : LowerCase Filter (TokenFilter), +, +## CharFilter : kuromoji_iteration_mark, +, +A charfilter of type `kuromoji_iteration_mark`., +This charfilter is Normalizes Japanese horizontal iteration marks (odoriji) to their expanded form., +, +The following ar setting that can be set for a `kuromoji_iteration_mark` charfilter type:, +, +| **Setting**     | **Description**                                              | **Default value** |, +|:----------------|:-------------------------------------------------------------|:------------------|, +| normalize_kanji | indicates whether kanji iteration marks should be normalized | `true`            |, +| normalize_kana  | indicates whether kanji iteration marks should be normalized | `true`            |, +, +## Tokenizer : kuromoji_tokenizer, +, +A tokenizer of type `kuromoji_tokenizer`., +, +The following are settings that can be set for a `kuromoji_tokenizer` tokenizer type:, +, +| **Setting**         | **Description**                                                                                                           | **Default value** |, +|:--------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------|, +| mode                | Tokenization mode: this determines how the tokenizer handles compound and unknown words. `normal` and `search`, `extended`| `search`          |, +| discard_punctuation | `true` if punctuation tokens should be dropped from the output.                                                           | `true`            |, +| user_dict           | set User Dictionary file                                                                                                  |                   |, +, +### Tokenization mode, +, +The mode is three types., +, +* `normal` : Ordinary segmentation: no decomposition for compounds, +, +* `search` : Segmentation geared towards search: this includes a decompounding process for long nouns, also including the full compound token as a synonym., +, +* `extended` : Extended mode outputs unigrams for unknown words., +, +#### Difference tokenization mode outputs, +, +Input text is `関西国際空港` and `アブラカダブラ`., +, +| **mode**   | `関西国際空港` | `アブラカダブラ` |]