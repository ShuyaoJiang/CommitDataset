[+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/GlobalOrdinalsSignificantTermsAggregator.java, +/*, + * Licensed to Elasticsearch under one or more contributor, + * license agreements. See the NOTICE file distributed with, + * this work for additional information regarding copyright, + * ownership. Elasticsearch licenses this file to you under, + * the Apache License, Version 2.0 (the "License"); you may, + * not use this file except in compliance with the License., + * You may obtain a copy of the License at, + *, + *    http://www.apache.org/licenses/LICENSE-2.0, + *, + * Unless required by applicable law or agreed to in writing,, + * software distributed under the License is distributed on an, + * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY, + * KIND, either express or implied.  See the License for the, + * specific language governing permissions and limitations, + * under the License., + */, +package org.elasticsearch.search.aggregations.bucket.significant;, +, +import org.apache.lucene.index.AtomicReaderContext;, +import org.apache.lucene.index.IndexReader;, +import org.apache.lucene.util.BytesRef;, +import org.elasticsearch.common.lease.Releasables;, +import org.elasticsearch.common.util.LongHash;, +import org.elasticsearch.index.fielddata.ordinals.Ordinals;, +import org.elasticsearch.search.aggregations.Aggregator;, +import org.elasticsearch.search.aggregations.AggregatorFactories;, +import org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator;, +import org.elasticsearch.search.aggregations.support.AggregationContext;, +import org.elasticsearch.search.aggregations.support.ValuesSource;, +import org.elasticsearch.search.internal.ContextIndexSearcher;, +, +import java.io.IOException;, +import java.util.Arrays;, +import java.util.Collections;, +, +/**, + * An global ordinal based implementation of significant terms, based on {@link SignificantStringTermsAggregator}., + */, +public class GlobalOrdinalsSignificantTermsAggregator extends GlobalOrdinalsStringTermsAggregator {, +, +    protected long numCollectedDocs;, +    protected final SignificantTermsAggregatorFactory termsAggFactory;, +, +    public GlobalOrdinalsSignificantTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Bytes.WithOrdinals.FieldData valuesSource,, +                                                    long estimatedBucketCount, long maxOrd, int requiredSize, int shardSize, long minDocCount,, +                                                    AggregationContext aggregationContext, Aggregator parent,, +                                                    SignificantTermsAggregatorFactory termsAggFactory) {, +, +        super(name, factories, valuesSource, estimatedBucketCount, maxOrd, null, requiredSize, shardSize,, +                minDocCount, aggregationContext, parent);, +        this.termsAggFactory = termsAggFactory;, +    }, +, +    @Override, +    public void collect(int doc, long owningBucketOrdinal) throws IOException {, +        super.collect(doc, owningBucketOrdinal);, +        numCollectedDocs++;, +    }, +, +    @Override, +    public SignificantStringTerms buildAggregation(long owningBucketOrdinal) {, +        assert owningBucketOrdinal == 0;, +        if (globalOrdinals == null) { // no context in this reader, +            return buildEmptyAggregation();, +        }, +, +        final int size;, +        if (minDocCount == 0) {, +            // if minDocCount == 0 then we can end up with more buckets then maxBucketOrd() returns, +            size = (int) Math.min(globalOrdinals.getMaxOrd(), shardSize);, +        } else {, +            size = (int) Math.min(maxBucketOrd(), shardSize);, +        }, +        long supersetSize = termsAggFactory.prepareBackground(context);, +        long subsetSize = numCollectedDocs;, +, +        BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size);, +        SignificantStringTerms.Bucket spare = null;, +        for (long termOrd = Ordinals.MIN_ORDINAL; termOrd < globalOrdinals.getMaxOrd(); ++termOrd) {, +            final long bucketOrd = getBucketOrd(termOrd);, +            final long bucketDocCount = bucketOrd < 0 ? 0 : bucketDocCount(bucketOrd);, +            if (minDocCount > 0 && bucketDocCount == 0) {, +                continue;, +            }, +            if (spare == null) {, +                spare = new SignificantStringTerms.Bucket(new BytesRef(), 0, 0, 0, 0, null);, +            }, +            spare.bucketOrd = bucketOrd;, +            copy(globalValues.getValueByOrd(termOrd), spare.termBytes);, +            spare.subsetDf = bucketDocCount;, +            spare.subsetSize = subsetSize;, +            spare.supersetDf = termsAggFactory.getBackgroundFrequency(spare.termBytes);, +            spare.supersetSize = supersetSize;, +            assert spare.subsetDf <= spare.supersetDf;, +            // During shard-local down-selection we use subset/superset stats, +            // that are for this shard only, +            // Back at the central reducer these properties will be updated with]