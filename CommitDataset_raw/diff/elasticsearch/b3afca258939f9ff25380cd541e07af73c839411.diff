[+++ b/modules/elasticsearch/src/main/java/org/elasticsearch/common/compress/lzf/ChunkEncoder.java, +/*, + * Licensed to Elastic Search and Shay Banon under one, + * or more contributor license agreements.  See the NOTICE file, + * distributed with this work for additional information, + * regarding copyright ownership. Elastic Search licenses this, + * file to you under the Apache License, Version 2.0 (the, + * "License"); you may not use this file except in compliance, + * with the License.  You may obtain a copy of the License at, + *, + *    http://www.apache.org/licenses/LICENSE-2.0, + *, + * Unless required by applicable law or agreed to in writing,, + * software distributed under the License is distributed on an, + * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY, + * KIND, either express or implied.  See the License for the, + * specific language governing permissions and limitations, + * under the License., + */, +, +/* Licensed under the Apache License, Version 2.0 (the "License"); you may not use this, + * file except in compliance with the License. You may obtain a copy of the License at, + *, + * http://www.apache.org/licenses/LICENSE-2.0, + *, + * Unless required by applicable law or agreed to in writing, software distributed under, + * the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS, + * OF ANY KIND, either express or implied. See the License for the specific language, + * governing permissions and limitations under the License., + */, +, +package org.elasticsearch.common.compress.lzf;, +, +/**, + * Class that handles actual encoding of individual chunks., + * Resulting chunks can be compressed or non-compressed; compression, + * is only used if it actually reduces chunk size (including overhead, + * of additional header bytes), + *, + * @author tatu@ning.com, + */, +public class ChunkEncoder {, +    // Beyond certain point we won't be able to compress; let's use 16 bytes as cut-off, +    private static final int MIN_BLOCK_TO_COMPRESS = 16;, +, +    private static final int MIN_HASH_SIZE = 256;, +, +    // Not much point in bigger tables, with 8k window, +    private static final int MAX_HASH_SIZE = 16384;, +, +    private static final int MAX_OFF = 1 << 13; // 8k, +    private static final int MAX_REF = (1 << 8) + (1 << 3); // 264, +, +    // // Encoding tables, +, +    /**, +     * Buffer in which encoded content is stored during processing, +     */, +    private final byte[] _encodeBuffer;, +, +    private final int[] _hashTable;, +, +    private final int _hashModulo;, +, +    /**, +     * @param totalLength Total encoded length; used for calculating size, +     *                    of hash table to use, +     */, +    public ChunkEncoder(int totalLength) {, +        int largestChunkLen = Math.max(totalLength, LZFChunk.MAX_CHUNK_LEN);, +, +        int hashLen = calcHashLen(largestChunkLen);, +        _hashTable = new int[hashLen];, +        _hashModulo = hashLen - 1;, +        // Ok, then, what's the worst case output buffer length?, +        // length indicator for each 32 literals, so:, +        int bufferLen = largestChunkLen + ((largestChunkLen + 31) >> 5);, +        _encodeBuffer = new byte[bufferLen];, +    }, +, +    /**, +     * Method for compressing (or not) individual chunks, +     */, +    public LZFChunk encodeChunk(byte[] data, int offset, int len) {, +        if (len >= MIN_BLOCK_TO_COMPRESS) {, +            /* If we have non-trivial block, and can compress it by at least, +             * 2 bytes (since header is 2 bytes longer), let's compress:, +             */, +            int compLen = tryCompress(data, offset, offset + len, _encodeBuffer, 0);, +            if (compLen < (len - 2)) { // nah; just return uncompressed, +                return LZFChunk.createCompressed(len, _encodeBuffer, 0, compLen);, +            }, +        }, +        // Otherwise leave uncompressed:, +        return LZFChunk.createNonCompressed(data, offset, len);, +    }, +, +    private static int calcHashLen(int chunkSize) {, +        // in general try get hash table size of 2x input size, +        chunkSize += chunkSize;]