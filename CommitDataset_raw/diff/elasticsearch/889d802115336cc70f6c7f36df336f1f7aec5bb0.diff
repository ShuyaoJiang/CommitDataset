[+++ b/benchmarks/README.md, +[Rally](http://github.com/elastic/rally). Microbenchmarks are intended to spot performance regressions in performance-critical components. , +The microbenchmark suite is also handy for ad-hoc microbenchmarks but please remove them again before merging your PR., +If you want to run a specific benchmark class, e.g. `org.elasticsearch.benchmark.MySampleBenchmark` or have special requirements , +generate the uberjar with `gradle :benchmarks:jmhJar` and run it directly with:, +JMH supports lots of command line parameters. Add `-h` to the command above to see the available command line options., +To get realistic results, you should exercise care when running benchmarks. Here are a few tips:, +* Ensure that the system executing your microbenchmarks has as little load as possible. Shutdown every process that can cause unnecessary , +* Ensure to run enough warmup iterations to get the benchmark into a stable state. If you are unsure, don't change the defaults., +* Vary the problem input size with `@Param`., +    * Run the generated uberjar directly and use `-prof gc` to check whether the garbage collector runs during a microbenchmarks and skews , +   your results. If so, try to force a GC between runs (`-gc true`) but watch out for the caveats., +    * Use `-prof perf` or `-prof perfasm` (both only available on Linux) to see hotspots., +* Blindly believe the numbers that your microbenchmark produces but verify them by measuring e.g. with `-prof perfasm`., +* Run more threads than your number of CPU cores (in case you run multi-threaded microbenchmarks).]