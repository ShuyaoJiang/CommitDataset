[+++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsPlugin.java, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsPlugin.java, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobStore.java, +    public HdfsBlobStore(Settings settings, FileContextFactory fcf, Path path, ThreadPool threadPool) throws IOException {, +        this.fcf = fcf;, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsPlugin.java, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobStore.java, +    public HdfsBlobStore(Settings settings, FileContextFactory fcf, Path path, ThreadPool threadPool) throws IOException {, +        this.fcf = fcf;, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java, +import org.apache.hadoop.conf.Configuration;, +import org.apache.hadoop.fs.AbstractFileSystem;, +import org.apache.hadoop.fs.FileContext;, +import org.apache.hadoop.fs.Path;, +import org.apache.hadoop.security.UserGroupInformation;, +import org.elasticsearch.ElasticsearchException;, +import org.elasticsearch.ElasticsearchGenerationException;, +import org.elasticsearch.SpecialPermission;, +import org.elasticsearch.common.Strings;, +import org.elasticsearch.common.blobstore.BlobPath;, +import org.elasticsearch.common.blobstore.BlobStore;, +import org.elasticsearch.common.inject.Inject;, +import org.elasticsearch.common.io.PathUtils;, +import org.elasticsearch.common.unit.ByteSizeValue;, +import org.elasticsearch.index.snapshots.IndexShardRepository;, +import org.elasticsearch.repositories.RepositoryName;, +import org.elasticsearch.repositories.RepositorySettings;, +import org.elasticsearch.repositories.blobstore.BlobStoreRepository;, +import org.elasticsearch.threadpool.ThreadPool;, +, +    private final String uri;, +        uri = repositorySettings.settings().get("uri", settings.get("uri"));, +        if (!Strings.hasText(uri)) {, +            throw new IllegalArgumentException("No 'uri' defined for hdfs snapshot/restore");, +        }, +, +        URI actualUri = URI.create(uri);, +        String scheme = actualUri.getScheme();, +        if (!Strings.hasText(scheme) || !scheme.toLowerCase(Locale.ROOT).equals("hdfs")) {, +            throw new IllegalArgumentException(, +                    String.format(Locale.ROOT, "Invalid scheme [%s] specified in uri [%s]; only 'hdfs' uri allowed for hdfs snapshot/restore", scheme, uri));, +        }, +        String p = actualUri.getPath();, +        if (Strings.hasText(p) && !p.equals("/")) {, +            throw new IllegalArgumentException(String.format(Locale.ROOT,, +                    "Use 'path' option to specify a path [%s], not the uri [%s] for hdfs snapshot/restore", p, uri));, +        }, +, +            throw new IllegalArgumentException("No 'path' defined for hdfs snapshot/restore");, +        URI actualUri = URI.create(uri);, +            cfg.setBoolean("fs.hdfs.impl.disable.cache", true);, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsPlugin.java, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobStore.java, +    public HdfsBlobStore(Settings settings, FileContextFactory fcf, Path path, ThreadPool threadPool) throws IOException {, +        this.fcf = fcf;, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsRepository.java, +import org.apache.hadoop.conf.Configuration;, +import org.apache.hadoop.fs.AbstractFileSystem;, +import org.apache.hadoop.fs.FileContext;, +import org.apache.hadoop.fs.Path;, +import org.apache.hadoop.security.UserGroupInformation;, +import org.elasticsearch.ElasticsearchException;, +import org.elasticsearch.ElasticsearchGenerationException;, +import org.elasticsearch.SpecialPermission;, +import org.elasticsearch.common.Strings;, +import org.elasticsearch.common.blobstore.BlobPath;, +import org.elasticsearch.common.blobstore.BlobStore;, +import org.elasticsearch.common.inject.Inject;, +import org.elasticsearch.common.io.PathUtils;, +import org.elasticsearch.common.unit.ByteSizeValue;, +import org.elasticsearch.index.snapshots.IndexShardRepository;, +import org.elasticsearch.repositories.RepositoryName;, +import org.elasticsearch.repositories.RepositorySettings;, +import org.elasticsearch.repositories.blobstore.BlobStoreRepository;, +import org.elasticsearch.threadpool.ThreadPool;, +, +    private final String uri;, +        uri = repositorySettings.settings().get("uri", settings.get("uri"));, +        if (!Strings.hasText(uri)) {, +            throw new IllegalArgumentException("No 'uri' defined for hdfs snapshot/restore");]