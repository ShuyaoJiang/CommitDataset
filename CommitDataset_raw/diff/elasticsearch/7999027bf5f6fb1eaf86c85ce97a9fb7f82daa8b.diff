[+++ b/core/src/main/java/org/elasticsearch/common/cache/Cache.java, +        Map<K, Entry<K, V>> map = new HashMap<>();, +            Entry<K, V> entry;, +                entry = map.get(key);, +            if (entry != null) {, +                existing = map.put(key, entry);, +            Entry<K, V> entry;, +                entry = map.remove(key);, +            if (entry != null) {, +     * value using the given mapping function and enters it into this map unless null., +            // we synchronize against the segment lock; this is to avoid a scenario where another thread is inserting, +            // a value for the same key via put which would not be observed on this thread without a mechanism, +            // synchronizing the two threads; it is possible that the segment lock will be too expensive here (it blocks, +            // readers too!) so consider this as a possible place to optimize should contention be observed, +                value = get(key, now);, +                if (value == null) {, +                        value = loader.load(key);, +                    } catch (Exception e) {, +                        throw new ExecutionException(e);, +                    if (value == null) {, +                    put(key, value, now);, +            }, +++ b/core/src/main/java/org/elasticsearch/common/cache/Cache.java, +        Map<K, Entry<K, V>> map = new HashMap<>();, +            Entry<K, V> entry;, +                entry = map.get(key);, +            if (entry != null) {, +                existing = map.put(key, entry);, +            Entry<K, V> entry;, +                entry = map.remove(key);, +            if (entry != null) {, +     * value using the given mapping function and enters it into this map unless null., +            // we synchronize against the segment lock; this is to avoid a scenario where another thread is inserting, +            // a value for the same key via put which would not be observed on this thread without a mechanism, +            // synchronizing the two threads; it is possible that the segment lock will be too expensive here (it blocks, +            // readers too!) so consider this as a possible place to optimize should contention be observed, +                value = get(key, now);, +                if (value == null) {, +                        value = loader.load(key);, +                    } catch (Exception e) {, +                        throw new ExecutionException(e);, +                    if (value == null) {, +                    put(key, value, now);, +            }, +++ b/core/src/main/java/org/elasticsearch/common/lucene/index/ElasticsearchDirectoryReader.java, +import org.apache.lucene.index.*;, +import org.elasticsearch.common.SuppressForbidden;, +    /**, +     * Adds the given listener to the provided directory reader. The reader must contain an {@link ElasticsearchDirectoryReader} in it's hierarchy, +     * otherwise we can't safely install the listener., +     *, +     * @throws IllegalArgumentException if the reader doesn't contain an {@link ElasticsearchDirectoryReader} in it's hierarchy, +     */, +    @SuppressForbidden(reason = "This is the only sane way to add a ReaderClosedListener"), +    public static void addReaderCloseListener(DirectoryReader reader, IndexReader.ReaderClosedListener listener) {, +        ElasticsearchDirectoryReader elasticsearchDirectoryReader = getElasticsearchDirectoryReader(reader);, +        if (elasticsearchDirectoryReader != null) {, +            assert reader.getCoreCacheKey() == elasticsearchDirectoryReader.getCoreCacheKey();, +            elasticsearchDirectoryReader.addReaderClosedListener(listener);, +            return;, +        }, +        throw new IllegalArgumentException("Can't install close listener reader is not an ElasticsearchDirectoryReader/ElasticsearchLeafReader");, +    }, +, +    /**, +     * Tries to unwrap the given reader until the first {@link ElasticsearchDirectoryReader} instance is found or <code>null</code> if no instance is found;, +     */, +    public static ElasticsearchDirectoryReader getElasticsearchDirectoryReader(DirectoryReader reader) {, +        if (reader instanceof FilterDirectoryReader) {, +            if (reader instanceof ElasticsearchDirectoryReader) {, +                return (ElasticsearchDirectoryReader) reader;, +            } else {, +                // We need to use FilterDirectoryReader#getDelegate and not FilterDirectoryReader#unwrap, because, +                // If there are multiple levels of filtered leaf readers then with the unwrap() method it immediately, +                // returns the most inner leaf reader and thus skipping of over any other filtered leaf reader that, +                // may be instance of ElasticsearchLeafReader. This can cause us to miss the shardId., +                return getElasticsearchDirectoryReader(((FilterDirectoryReader) reader).getDelegate());, +            }, +        }, +        return null;, +    }, +++ b/core/src/main/java/org/elasticsearch/common/cache/Cache.java, +        Map<K, Entry<K, V>> map = new HashMap<>();, +            Entry<K, V> entry;, +                entry = map.get(key);, +            if (entry != null) {, +                existing = map.put(key, entry);, +            Entry<K, V> entry;, +                entry = map.remove(key);, +            if (entry != null) {, +     * value using the given mapping function and enters it into this map unless null., +            // we synchronize against the segment lock; this is to avoid a scenario where another thread is inserting, +            // a value for the same key via put which would not be observed on this thread without a mechanism, +            // synchronizing the two threads; it is possible that the segment lock will be too expensive here (it blocks, +            // readers too!) so consider this as a possible place to optimize should contention be observed, +                value = get(key, now);, +                if (value == null) {, +                        value = loader.load(key);, +                    } catch (Exception e) {, +                        throw new ExecutionException(e);]