[+++ b/docs/plugins/repository-hdfs.asciidoc, +`security.principal`::, +, +    Kerberos principal to use when connecting to a secured HDFS cluster., +    If you are using a service principal for your elasticsearch node, you may, +    use the `_HOST` pattern in the principal name and the plugin will replace, +    the pattern with the hostname of the node at runtime (see, +    link:repository-hdfs-security-runtime[Creating the Secure Repository])., +, +[[repository-hdfs-security]], +==== Hadoop Security, +, +The HDFS Repository Plugin integrates seamlessly with Hadoop's authentication model. The following authentication, +methods are supported by the plugin:, +, +[horizontal], +`simple`::, +, +    Also means "no security" and is enabled by default. Uses information from underlying operating system account, +    running elasticsearch to inform Hadoop of the name of the current user. Hadoop makes no attempts to verify this, +    information., +, +`kerberos`::, +, +    Authenticates to Hadoop through the usage of a Kerberos principal and keytab. Interfacing with HDFS clusters, +    secured with Kerberos requires a few additional steps to enable (See <<repository-hdfs-security-keytabs>> and, +    <<repository-hdfs-security-runtime>> for more info), +, +[[repository-hdfs-security-keytabs]], +[float], +===== Principals and Keytabs, +Before attempting to connect to a secured HDFS cluster, provision the Kerberos principals and keytabs that the, +Elasticsearch nodes will use for authenticating to Kerberos. For maximum security and to avoid tripping up the Kerberos, +replay protection, you should create a service principal per node, following the pattern of, +`elasticsearch/hostname@REALM`., +, +WARNING: In some cases, if the same principal is authenticating from multiple clients at once, services may reject, +authentication for those principals under the assumption that they could be replay attacks. If you are running the, +plugin in production with multiple nodes you should be using a unique service principal for each node., +, +On each Elasticsearch node, place the appropriate keytab file in the node's configuration location under the, +`repository-hdfs` directory using the name `krb5.keytab`:, +, +[source, bash], +----, +$> cd elasticsearch/config, +$> ls, +elasticsearch.yml  jvm.options        log4j2.properties  repository-hdfs/   scripts/, +$> cd repository-hdfs, +$> ls, +krb5.keytab, +----, +// TEST[skip:this is for demonstration purposes only, +, +NOTE: Make sure you have the correct keytabs! If you are using a service principal per node (like, +`elasticsearch/hostname@REALM`) then each node will need its own unique keytab file for the principal assigned to that, +host!, +, +// Setup at runtime (principal name), +[[repository-hdfs-security-runtime]], +[float], +===== Creating the Secure Repository, +Once your keytab files are in place and your cluster is started, creating a secured HDFS repository is simple. Just, +add the name of the principal that you will be authenticating as in the repository settings under the, +`security.principal` option:, +, +[source,js], +----, +PUT _snapshot/my_hdfs_repository, +{, +  "type": "hdfs",, +  "settings": {, +    "uri": "hdfs://namenode:8020/",, +    "path": "/user/elasticsearch/respositories/my_hdfs_repository",, +    "security.principal": "elasticsearch@REALM", +  }, +}, +----, +// CONSOLE, +// TEST[skip:we don't have hdfs set up while testing this], +, +If you are using different service principals for each node, you can use the `_HOST` pattern in your principal, +name. Elasticsearch will automatically replace the pattern with the hostname of the node at runtime:, +, +[source,js], +----, +PUT _snapshot/my_hdfs_repository, +{, +  "type": "hdfs",, +  "settings": {, +    "uri": "hdfs://namenode:8020/",, +    "path": "/user/elasticsearch/respositories/my_hdfs_repository",, +    "security.principal": "elasticsearch/_HOST@REALM", +  }, +}, +----, +// CONSOLE, +// TEST[skip:we don't have hdfs set up while testing this], +, +[[repository-hdfs-security-authorization]]]