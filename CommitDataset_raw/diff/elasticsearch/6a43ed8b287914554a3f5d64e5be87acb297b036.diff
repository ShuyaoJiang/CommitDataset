[+++ b/docs/resiliency/index.asciidoc, +=== Better request retry mechanism when nodes are disconnected (STATUS: ONGOING), +, +If the node holding a primary shard is disconnected for whatever reason, the, +coordinating node retries the request on the same or a new primary shard.  In, +certain rare conditions, where the node disconnects and immediately, +reconnects, it is possible that the original request has already been, +successfully applied but has not been reported, resulting in duplicate, +requests. This is particularly true when retrying bulk requests, where some, +actions may  have completed and some may not have., +, +An optimization which disabled the existence check for documents indexed with, +auto-generated IDs could result in the creation of duplicate documents. This, +optimization has been removed. {GIT}9468[#9468] (STATUS: DONE, v1.5.0), +, +Further issues remain with the retry mechanism:, +, +* Unversioned index requests could increment the `_version` twice,, +  obscuring a `created` status., +* Versioned index requests could return a conflict exception, even, +  though they were applied correctly., +* Update requests could be applied twice., +, +See {GIT}9967[#9967]. (STATUS: ONGOING), +, +[float], +=== Write index metadata on data nodes where shards allocated (STATUS: ONGOING), +, +Today, index metadata is written only on nodes that are master-eligible, not on, +data-only nodes.  This is not a problem when running with multiple master nodes,, +as recommended, as the loss of all but one master node is still recoverable., +However, users running with a single master node are at risk of losing, +their index metadata if the master fails.  Instead, this metadata should, +also be written on any node where a shard is allocated. {GIT}8823[#8823], +, +[float], +=== Better file distribution with multiple data paths (STATUS: ONGOING), +, +Today, a node configured with multiple data paths distributes writes across, +all paths by writing one file to each path in turn.  This can mean that the, +failure of a single disk corrupts many shards at once.  Instead, by allocating, +an entire shard to a single data path, the extent of the damage can be limited, +to just the shards on that disk. {GIT}9498[#9498], +, +[float], +=== OOM resiliency (STATUS: ONGOING), +, +The family of circuit breakers has greatly reduced the occurrence of OOM, +exceptions, but it is still possible to cause a node to run out of heap, +space.  The following issues have been identified:, +, +* Set a hard limit on `from`/`size` parameters {GIT}9311[#9311]. (STATUS: ONGOING), +* Prevent combinatorial explosion in aggregations from causing OOM {GIT}8081[#8081]. (STATUS: ONGOING), +* Add the byte size of each hit to the request circuit breaker {GIT}9310[#9310]. (STATUS: ONGOING), +, +[float], +=== Mapping changes should be applied synchronously (STATUS: ONGOING), +, +When introducing new fields using dynamic mapping, it is possible that the same, +field can be added to different shards with different data types.  Each shard, +will operate with its local data type but, if the shard is relocated, the, +data type from the cluster state will be applied to the new shard, which, +can result in a corrupt shard.  To prevent this, new fields should not, +be added to a shard's mapping until confirmed by the master., +{GIT}8688[#8688] (STATUS: ONGOING), +, +[float], +=== Lucene checksums phase 3 (STATUS:ONGOING), +Almost all files in Elasticsearch now have checksums which are validated before use.  A few changes remain:, +* {GIT}7586[#7586] adds checksums for cluster and index state files. (STATUS: DONE, Fixed in v1.5.0), +* {GIT}9183[#9183] supports validating the checksums on all files when starting a node. (STATUS: DONE, Fixed in v2.0.0), +* {JIRA}5894[LUCENE-5894] lays the groundwork for extending more efficient checksum validation to all files during optimized bulk merges. (STATUS: ONGOING, Fixed in Lucene 5.0), +* {GIT}8403[#8403] to add validation of checksums on Lucene `segments_N` files. (STATUS: NOT STARTED), +=== Report shard-level statuses on write operations (STATUS: ONGOING), +Make write calls return the number of total/successful/missing shards in the same way that we do in search, which ensures transparency in the consistency of write operations. {GIT}7994[#7994]. (STATUS: DONE, v2.0.0), +=== Simplify and harden shard recovery and allocation (STATUS: ONGOING), +Randomized testing combined with chaotic failures has revealed corner cases, +where the recovery and allocation of shards in a concurrent manner can result, +in shard corruption.  There is an ongoing effort to reduce the complexity of, +these operations in order to make them more deterministic.  These include:, +* Introduce shard level locks to prevent concurrent shard modifications {GIT}8436[#8436]. (STATUS: DONE, Fixed in v1.5.0), +* Delete shard contents under a lock {GIT}9083[#9083]. (STATUS: DONE, Fixed in v1.5.0), +* Delete shard under a lock {GIT}8579[#8579]. (STATUS: DONE, Fixed in v1.5.0), +* Refactor RecoveryTarget state management {GIT}8092[#8092]. (STATUS: DONE, Fixed in v1.5.0), +* Cancelling a recovery may leave temporary files behind {GIT}7893[#7893]. (STATUS: DONE, Fixed in v1.5.0), +* Quick cluster state processing can result in both shard copies being deleted {GIT}9503[#9503]. (STATUS: DONE, Fixed in v1.5.0), +* Rapid creation and deletion of an index can cause reuse of old index metadata {GIT}9489[#9489]. (STATUS: DONE, Fixed in v1.5.0), +* Flush immediately after the last concurrent recovery finishes to clear out the translog before a new recovery starts {GIT}9439[#9439]. (STATUS: DONE, Fixed in v1.5.0), +, +[float], +=== Prevent setting minimum_master_nodes to more than the current node count (STATUS: ONGOING), +, +Setting `zen.discovery.minimum_master_nodes` to a value higher than the current node count, +effectively leaves the cluster without a master and unable to process requests.  The only, +way to fix this is to add more master-eligibile nodes.  {GIT}8321[#8321] adds a mechanism, +to validate settings before applying them, and {GIT}9051[#9051] extends this validation, +support to settings applied during a cluster restore. (STATUS: DONE, Fixed in v1.5.0), +While we are working on a longer term solution ({GIT}9176[#9176]), we introduced a minimum weight of 1k for each cache entry. This puts an effective limit on the number of entries in the cache. See {GIT}8304[#8304] (STATUS: DONE, fixed in v1.4.0), +, +[float]]