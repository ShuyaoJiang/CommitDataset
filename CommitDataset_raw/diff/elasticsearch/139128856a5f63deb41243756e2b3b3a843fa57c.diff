[+++ b/x-pack/plugin/watcher/src/main/java/org/elasticsearch/xpack/watcher/Watcher.java, +import org.apache.logging.log4j.LogManager;, +import org.elasticsearch.action.bulk.BulkItemResponse;, +import org.elasticsearch.action.bulk.BulkProcessor;, +import org.elasticsearch.action.bulk.BulkRequest;, +import org.elasticsearch.action.bulk.BulkResponse;, +import org.elasticsearch.common.unit.ByteSizeUnit;, +import org.elasticsearch.common.unit.ByteSizeValue;, +import org.elasticsearch.xpack.core.ClientHelper;, +import java.util.concurrent.TimeUnit;, +import java.util.stream.Collectors;, +import static org.elasticsearch.common.settings.Setting.Property.NodeScope;, +import static org.elasticsearch.xpack.core.ClientHelper.WATCHER_ORIGIN;, +    private static final Setting<Integer> SETTING_BULK_ACTIONS =, +        Setting.intSetting("xpack.watcher.bulk.actions", 1, 1, 10000, NodeScope);, +    private static final Setting<Integer> SETTING_BULK_CONCURRENT_REQUESTS =, +        Setting.intSetting("xpack.watcher.bulk.concurrent_requests", 0, 0, 20, NodeScope);, +    private static final Setting<TimeValue> SETTING_BULK_FLUSH_INTERVAL =, +        Setting.timeSetting("xpack.watcher.bulk.flush_interval", TimeValue.timeValueSeconds(1), NodeScope);, +    private static final Setting<ByteSizeValue> SETTING_BULK_SIZE =, +        Setting.byteSizeSetting("xpack.watcher.bulk.size", new ByteSizeValue(1, ByteSizeUnit.MB),, +            new ByteSizeValue(1, ByteSizeUnit.MB), new ByteSizeValue(10, ByteSizeUnit.MB), NodeScope);, +, +    private static final Logger logger = LogManager.getLogger(Watcher.class);, +    private BulkProcessor bulkProcessor;, +        bulkProcessor = BulkProcessor.builder(ClientHelper.clientWithOrigin(client, WATCHER_ORIGIN), new BulkProcessor.Listener() {, +            @Override, +            public void beforeBulk(long executionId, BulkRequest request) {, +            }, +, +            @Override, +            public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {, +                if (response.hasFailures()) {, +                    Map<String, String> triggeredWatches = Arrays.stream(response.getItems()), +                        .filter(BulkItemResponse::isFailed), +                        .filter(r -> r.getIndex().startsWith(TriggeredWatchStoreField.INDEX_NAME)), +                        .collect(Collectors.toMap(BulkItemResponse::getId, BulkItemResponse::getFailureMessage));, +                    if (triggeredWatches.isEmpty() == false) {, +                        String failure = triggeredWatches.values().stream().collect(Collectors.joining(", "));, +                        logger.error("triggered watches could not be deleted {}, failure [{}]",, +                            triggeredWatches.keySet(), Strings.substring(failure, 0, 2000));, +                    }, +, +                    Map<String, String> overwrittenIds = Arrays.stream(response.getItems()), +                        .filter(BulkItemResponse::isFailed), +                        .filter(r -> r.getIndex().startsWith(HistoryStoreField.INDEX_PREFIX)), +                        .filter(r -> r.getVersion() > 1), +                        .collect(Collectors.toMap(BulkItemResponse::getId, BulkItemResponse::getFailureMessage));, +                    if (overwrittenIds.isEmpty() == false) {, +                        String failure = overwrittenIds.values().stream().collect(Collectors.joining(", "));, +                        logger.info("overwrote watch history entries {}, possible second execution of a triggered watch, failure [{}]",, +                            overwrittenIds.keySet(), Strings.substring(failure, 0, 2000));, +                    }, +                }, +            }, +, +            @Override, +            public void afterBulk(long executionId, BulkRequest request, Throwable failure) {, +                logger.error("error executing bulk", failure);, +            }, +        }), +            .setFlushInterval(SETTING_BULK_FLUSH_INTERVAL.get(settings)), +            .setBulkActions(SETTING_BULK_ACTIONS.get(settings)), +            .setBulkSize(SETTING_BULK_SIZE.get(settings)), +            .setConcurrentRequests(SETTING_BULK_CONCURRENT_REQUESTS.get(settings)), +            .build();, +, +        HistoryStore historyStore = new HistoryStore(settings, bulkProcessor);, +        final TriggeredWatchStore triggeredWatchStore = new TriggeredWatchStore(settings, client, triggeredWatchParser, bulkProcessor);, +        // bulk processor configuration, +        settings.add(SETTING_BULK_ACTIONS);, +        settings.add(SETTING_BULK_CONCURRENT_REQUESTS);, +        settings.add(SETTING_BULK_FLUSH_INTERVAL);, +        settings.add(SETTING_BULK_SIZE);, +, +        bulkProcessor.flush();, +        try {, +            if (bulkProcessor.awaitClose(10, TimeUnit.SECONDS) == false) {, +                logger.warn("failed to properly close watcher bulk processor");, +            }, +        } catch (InterruptedException e) {, +            Thread.currentThread().interrupt();, +        }, +++ b/x-pack/plugin/watcher/src/main/java/org/elasticsearch/xpack/watcher/Watcher.java, +import org.apache.logging.log4j.LogManager;, +import org.elasticsearch.action.bulk.BulkItemResponse;, +import org.elasticsearch.action.bulk.BulkProcessor;, +import org.elasticsearch.action.bulk.BulkRequest;, +import org.elasticsearch.action.bulk.BulkResponse;, +import org.elasticsearch.common.unit.ByteSizeUnit;, +import org.elasticsearch.common.unit.ByteSizeValue;, +import org.elasticsearch.xpack.core.ClientHelper;, +import java.util.concurrent.TimeUnit;, +import java.util.stream.Collectors;, +import static org.elasticsearch.common.settings.Setting.Property.NodeScope;, +import static org.elasticsearch.xpack.core.ClientHelper.WATCHER_ORIGIN;, +    private static final Setting<Integer> SETTING_BULK_ACTIONS =, +        Setting.intSetting("xpack.watcher.bulk.actions", 1, 1, 10000, NodeScope);, +    private static final Setting<Integer> SETTING_BULK_CONCURRENT_REQUESTS =, +        Setting.intSetting("xpack.watcher.bulk.concurrent_requests", 0, 0, 20, NodeScope);]