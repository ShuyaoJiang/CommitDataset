[+++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/get-datafeed-stats.asciidoc, +          "ml.max_open_jobs": "20", +++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/get-datafeed-stats.asciidoc, +          "ml.max_open_jobs": "20", +++ b/docs/reference/ml/apis/jobcounts.asciidoc, +  (object) For example, {"ml.machine_memory": "17179869184"}., +++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/get-datafeed-stats.asciidoc, +          "ml.max_open_jobs": "20", +++ b/docs/reference/ml/apis/jobcounts.asciidoc, +  (object) For example, {"ml.machine_memory": "17179869184"}., +++ b/docs/reference/settings/ml-settings.asciidoc, +, +`xpack.ml.max_machine_memory_percent` (<<cluster-update-settings,Dynamic>>)::, +`xpack.ml.max_model_memory_limit` (<<cluster-update-settings,Dynamic>>)::, +`xpack.ml.max_open_jobs` (<<cluster-update-settings,Dynamic>>)::, +The maximum number of jobs that can run simultaneously on a node. Defaults to, +`20`. In this context, jobs include both anomaly detector jobs and data frame, +analytics jobs. The maximum number of jobs is also constrained by memory usage., +Thus if the estimated memory usage of the jobs would be higher than allowed,, +fewer jobs will run on a node. Prior to version 7.1, this setting was a per-node, +non-dynamic setting. It became a cluster-wide dynamic, +setting in version 7.1. As a result, changes to its value after node startup, +are used only after every node in the cluster is running version 7.1 or higher., +The maximum permitted value is `512`., +`xpack.ml.node_concurrent_job_allocations` (<<cluster-update-settings,Dynamic>>)::, +++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/get-datafeed-stats.asciidoc, +          "ml.max_open_jobs": "20", +++ b/docs/reference/ml/apis/jobcounts.asciidoc, +  (object) For example, {"ml.machine_memory": "17179869184"}., +++ b/docs/reference/settings/ml-settings.asciidoc, +, +`xpack.ml.max_machine_memory_percent` (<<cluster-update-settings,Dynamic>>)::, +`xpack.ml.max_model_memory_limit` (<<cluster-update-settings,Dynamic>>)::, +`xpack.ml.max_open_jobs` (<<cluster-update-settings,Dynamic>>)::, +The maximum number of jobs that can run simultaneously on a node. Defaults to, +`20`. In this context, jobs include both anomaly detector jobs and data frame, +analytics jobs. The maximum number of jobs is also constrained by memory usage., +Thus if the estimated memory usage of the jobs would be higher than allowed,, +fewer jobs will run on a node. Prior to version 7.1, this setting was a per-node, +non-dynamic setting. It became a cluster-wide dynamic, +setting in version 7.1. As a result, changes to its value after node startup, +are used only after every node in the cluster is running version 7.1 or higher., +The maximum permitted value is `512`., +`xpack.ml.node_concurrent_job_allocations` (<<cluster-update-settings,Dynamic>>)::, +++ b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/MachineLearning.java, +import org.elasticsearch.threadpool.ScalingExecutorBuilder;, +    public static final String JOB_COMMS_THREAD_POOL_NAME = NAME + "_job_comms";, +    // Before 8.0.0 this needs to match the max allowed value for xpack.ml.max_open_jobs,, +    // as the current node could be running in a cluster where some nodes are still using, +    // that setting.  From 8.0.0 onwards we have the flexibility to increase it..., +    private static final int MAX_MAX_OPEN_JOBS_PER_NODE = 512;, +    // This setting is cluster-wide and can be set dynamically. However, prior to version 7.1 it was, +    // a non-dynamic per-node setting. n a mixed version cluster containing 6.7 or 7.0 nodes those, +    // older nodes will not react to the dynamic changes. Therefore, in such mixed version clusters, +    // allocation will be based on the value first read at node startup rather than the current value., +    public static final Setting<Integer> MAX_OPEN_JOBS_PER_NODE =, +            Setting.intSetting("xpack.ml.max_open_jobs", 20, 1, MAX_MAX_OPEN_JOBS_PER_NODE, Property.Dynamic, Property.NodeScope);, +, +                        MAX_OPEN_JOBS_PER_NODE,, +            // TODO: stop setting this attribute in 8.0.0 but disallow it (like mlEnabledNodeAttrName below), +            // The ML UI will need to be changed to check machineMemoryAttrName instead before this is done, +                    String.valueOf(MAX_OPEN_JOBS_PER_NODE.get(settings)));, +, +        // These thread pools scale such that they can accommodate the maximum number of jobs per node, +        // that is permitted to be configured.  It is up to other code to enforce the configured maximum, +        // number of jobs per node., +        // 4 threads per job process: for input, c++ logger output, result processing and state processing., +        ScalingExecutorBuilder jobComms = new ScalingExecutorBuilder(JOB_COMMS_THREAD_POOL_NAME,, +            4, MAX_MAX_OPEN_JOBS_PER_NODE * 4, TimeValue.timeValueMinutes(1), "xpack.ml.job_comms_thread_pool");, +, +        // This pool is used by renormalization, plus some other parts of ML that, +        // need to kick off non-trivial activities that mustn't block other threads., +        ScalingExecutorBuilder utility = new ScalingExecutorBuilder(UTILITY_THREAD_POOL_NAME,, +            1, MAX_MAX_OPEN_JOBS_PER_NODE * 4, TimeValue.timeValueMinutes(10), "xpack.ml.utility_thread_pool");, +, +        ScalingExecutorBuilder datafeed = new ScalingExecutorBuilder(DATAFEED_THREAD_POOL_NAME,, +            1, MAX_MAX_OPEN_JOBS_PER_NODE, TimeValue.timeValueMinutes(1), "xpack.ml.datafeed_thread_pool");, +, +        return Arrays.asList(jobComms, utility, datafeed);, +, +++ b/docs/reference/ml/apis/datafeedresource.asciidoc, +  `attributes`::: For example, `{"ml.machine_memory": "17179869184"}`., +++ b/docs/reference/ml/apis/get-datafeed-stats.asciidoc, +          "ml.max_open_jobs": "20", +++ b/docs/reference/ml/apis/jobcounts.asciidoc, +  (object) For example, {"ml.machine_memory": "17179869184"}., +++ b/docs/reference/settings/ml-settings.asciidoc, +, +`xpack.ml.max_machine_memory_percent` (<<cluster-update-settings,Dynamic>>)::, +`xpack.ml.max_model_memory_limit` (<<cluster-update-settings,Dynamic>>)::]