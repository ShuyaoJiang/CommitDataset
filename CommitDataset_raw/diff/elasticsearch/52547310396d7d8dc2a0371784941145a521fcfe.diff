[+++ b/core/src/main/java/org/elasticsearch/script/Script.java, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, Object> options = (Map)this.options;, +            Map<String, Object> options = (Map)this.options;, +++ b/core/src/main/java/org/elasticsearch/script/Script.java, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, Object> options = (Map)this.options;, +            Map<String, Object> options = (Map)this.options;, +++ b/docs/reference/search.asciidoc, +, +include::search/rank-eval.asciidoc[], +++ b/core/src/main/java/org/elasticsearch/script/Script.java, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, String> options = (Map)in.readMap();, +            Map<String, Object> options = (Map)this.options;, +            Map<String, Object> options = (Map)this.options;, +++ b/docs/reference/search.asciidoc, +, +include::search/rank-eval.asciidoc[], +++ b/docs/reference/search/rank-eval.asciidoc, +[[rank-eval]], += Ranking Evaluation, +, +[partintro], +--, +, +Imagine having built and deployed a search application: Users are happily, +entering queries into your search frontend. Your application takes these, +queries and creates a dedicated Elasticsearch query from that, and returns its, +results back to the user.  Imagine further that you are tasked with tweaking the, +Elasticsearch query that is being created to return specific results for a, +certain set of queries without breaking others. How should that be done?, +, +One possible solution is to gather a sample of user queries representative of, +how the search application is used, retrieve the search results that are being, +returned. As a next step these search results would be manually annotated for, +their relevancy to the original user query. Based on this set of rated requests, +we can compute a couple of metrics telling us more about how many relevant, +search results are being returned., +, +This is a nice approximation for how well our translation from user query to, +Elasticsearch query works for providing the user with relevant search results., +Elasticsearch provides a ranking evaluation API that lets you compute scores for, +your current ranking function based on annotated search results., +--, +, +== Plain ranking evaluation, +, +In its most simple form, for each query a set of ratings can be supplied:, +, +[source,js], +-----------------------------, +GET /twitter/tweet/_rank_eval, +{, +    "requests": [, +    {, +        "id": "JFK query",                              <1>, +        "request": {, +            "query": {, +                "match": {, +                    "title": {, +                        "query": "JFK"}}}},             <2>, +        "ratings": [                                    <3>, +        {, +          "rating": 1.5,                                <4>, +          "_type": "tweet",                             <5>, +          "_id": "13736278",                            <6>, +          "_index": "twitter"                           <7>, +        },, +        {, +          "rating": 1,, +          "_type": "tweet",, +          "_id": "30900421",, +          "_index": "twitter", +        }],  , +      "summary_fields": ["title"]                       <8>, +    }],, +    "metric": {                                         <9>, +      "reciprocal_rank": {}, +   },, +   "max_concurrent_searches": 10                         <10>, +}, +------------------------------, +// CONSOLE, +// TEST[setup:twitter], +, +<1> A human readable id for the rated query (that will be re-used in the response to provide further details)., +<2> The actual Elasticsearch query to execute., +<3> A set of ratings for how well a certain document fits as response for the query., +<4> A rating expressing how well the document fits the query, higher is better, are treated as int values., +<5> The type where the rated document lives., +<6> The id of the rated document., +<7> The index where the rated document lives., +<8> For a verbose response, specify which properties of a search hit should be returned in addition to index/type/id., +<9> A metric to use for evaluation. See below for a list., +<10> Maximum number of search requests to execute in parallel. Set to 10 by, +default., +]