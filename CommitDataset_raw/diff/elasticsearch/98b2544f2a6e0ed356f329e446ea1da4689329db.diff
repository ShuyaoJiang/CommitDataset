[+++ b/.gitignore, +/.project, +++ b/.gitignore, +/.project, +++ b/CONTRIBUTING.md, +**Repository:** [https://github.com/elasticsearch/elasticsearch-analysis-kuromoji](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji), +cd elasticsearch-analysis-kuromoji/, +++ b/.gitignore, +/.project, +++ b/CONTRIBUTING.md, +**Repository:** [https://github.com/elasticsearch/elasticsearch-analysis-kuromoji](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji), +cd elasticsearch-analysis-kuromoji/, +++ b/README.md, +Japanese (kuromoji) Analysis for ElasticSearch, +The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis module into elasticsearch., +In order to install the plugin, simply run: `bin/plugin -install elasticsearch/elasticsearch-analysis-kuromoji/1.6.0`., +| Kuromoji Analysis Plugin | elasticsearch    | Release date |, +| 1.7.0-SNAPSHOT (master)  | 0.90.8 -> master |  2013-12-19  |, +| 1.6.0                    | 0.90.6 -> 0.90.7 |  2013-11-06  |, +| 1.5.0                    | 0.90.3 -> 0.90.5 |  2013-08-08  |, +| 1.4.0                    | 0.90.1 -> 0.90.2 |  2013-05-30  |, +| 1.3.0                    | 0.90.0           |  2013-04-29  |, +| 1.2.0                    | 0.90.0           |  2013-02-26  |, +| 1.1.0                    | 0.19.2 -> 0.20   |  2012-11-21  |, +| 1.0.0                    | 0.19.0 -> 0.19.1 |  2012-04-30  |, +The plugin includes the `kuromoji` analyzer., +, +Includes Analyzer, Tokenizer, TokenFilter, +----------------------------------------, +, +The plugin includes these analyzer and tokenizer, tokenfilter., +, +| name                    | type        |, +|-------------------------|-------------|, +| kuromoji_iteration_mark | charfilter  |, +| kuromoji                | analyzer    |, +| kuromoji_tokenizer      | tokenizer   |, +| kuromoji_baseform       | tokenfilter |, +| kuromoji_part_of_speech | tokenfilter |, +| kuromoji_readingform    | tokenfilter |, +| kuromoji_stemmer        | tokenfilter |, +, +, +Usage, +-----, +, +## Analyzer : kuromoji, +, +An analyzer of type `kuromoji`., +This analyzer is the following tokenizer and tokenfilter combination., +, +* `kuromoji_tokenizer` : Kuromoji Tokenizer, +* `kuromoji_baseform` : Kuromoji BasicFormFilter (TokenFilter), +* `kuromoji_part_of_speech` : Kuromoji Part of Speech Stop Filter (TokenFilter), +* `cjk_width` : CJK Width Filter (TokenFilter), +* `stop` : Stop Filter (TokenFilter), +* `kuromoji_stemmer` : Kuromiji Katakana Stemmer Filter(TokenFilter), +* `lowercase` : LowerCase Filter (TokenFilter), +, +## CharFilter : kuromoji_iteration_mark, +, +A charfilter of type `kuromoji_iteration_mark`., +This charfilter is Normalizes Japanese horizontal iteration marks (odoriji) to their expanded form., +, +The following ar setting that can be set for a `kuromoji_iteration_mark` charfilter type:, +, +| **Setting**     | **Description**                                              | **Default value** |, +|:----------------|:-------------------------------------------------------------|:------------------|, +| normalize_kanji | indicates whether kanji iteration marks should be normalized | `true`            |, +| normalize_kana  | indicates whether kanji iteration marks should be normalized | `true`            |, +, +## Tokenizer : kuromoji_tokenizer, +, +A tokenizer of type `kuromoji_tokenizer`., +, +The following are settings that can be set for a `kuromoji_tokenizer` tokenizer type:, +, +| **Setting**         | **Description**                                                                                                           | **Default value** |, +|:--------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------|, +| mode                | Tokenization mode: this determines how the tokenizer handles compound and unknown words. `normal` and `search`, `extended`| `search`          |, +| discard_punctuation | `true` if punctuation tokens should be dropped from the output.                                                           | `true`            |, +| user_dict           | set User Dictionary file                                                                                                  |                   |, +, +### Tokenization mode, +, +The mode is three types., +, +* `normal` : Ordinary segmentation: no decomposition for compounds, +, +* `search` : Segmentation geared towards search: this includes a decompounding process for long nouns, also includeing the full compound token as a synonym., +, +* `extended` : Extended mode outputs unigrams for unknown words., +, +#### Difference tokenization mode outputs, +, +Input text is `関西国際空港` and `アブラカダブラ`., +, +| **mode**   | `関西国際空港` | `アブラカダブラ` |, +|:-----------|:-------------|:-------|, +| `normal`   | `関西国際空港` | `アブラカダブラ` |]