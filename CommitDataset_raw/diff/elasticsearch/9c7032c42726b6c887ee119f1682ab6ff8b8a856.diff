[+++ b/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java, +import org.apache.lucene.util.BytesRef;, +import org.elasticsearch.common.util.BigArrays;, +import org.elasticsearch.common.util.ByteArray;, + * A @link {@link StreamOutput} that uses{@link BigArrays} to acquire pages of, + * bytes, which avoids frequent reallocation & copying of the internal data., +    /**, +     * Factory/manager for our ByteArray, +     */, +    private final BigArrays bigarrays;, +     * The internal list of pages., +    private ByteArray bytes;, +    private int count;, +    /**, +     * Create a nonrecycling {@link BytesStreamOutput} with 1 initial page acquired., +     */, +        this(BigArrays.PAGE_SIZE_IN_BYTES);, +    /**, +     * Create a nonrecycling {@link BytesStreamOutput} with enough initial pages acquired, +     * to satisfy the capacity given by {@link expectedSize}., +     * , +     * @param expectedSize the expected maximum size of the stream in bytes., +     */, +    public BytesStreamOutput(int expectedSize) {, +        bigarrays = BigArrays.NON_RECYCLING_INSTANCE;, +        bytes = bigarrays.newByteArray(expectedSize);, +        ensureCapacity(count+1);, +        bytes.set(count, b);, +        count++;, +        // nothing to copy, +, +        // illegal args: offset and/or length exceed array size, +        if (b.length < (offset + length)) {, +            throw new IllegalArgumentException("Illegal offset " + offset + "/length " + length + " for byte[] of length " + b.length);, +        // get enough pages for new size, +        ensureCapacity(count+length);, +        // bulk copy, +        bytes.set(count, b, offset, length);, +, +        // advance, +        count += length;, +        // shrink list of pages, +        if (bytes.size() > BigArrays.PAGE_SIZE_IN_BYTES) {, +            bytes = bigarrays.resize(bytes, BigArrays.PAGE_SIZE_IN_BYTES);, +        // go back to start, +        count = 0;, +        // nothing to do, +    }, +, +    @Override, +    public void seek(long position) throws IOException {, +        if (position > Integer.MAX_VALUE) {, +            throw new IllegalArgumentException("position " + position + " > Integer.MAX_VALUE");, +        }, +, +        count = (int)position;, +        ensureCapacity(count);, +    }, +, +    public void skip(int length) {, +        count += length;, +        ensureCapacity(count);, +        // empty for now., +     * @return the value of the <code>count</code> field, which is the number of valid, +     *         bytes in this output stream., +, +    @Override, +    public BytesReference bytes() {, +        BytesRef bref = new BytesRef();, +        bytes.get(0, count, bref);, +        return new BytesArray(bref, false);, +    }, +, +    private void ensureCapacity(int offset) {, +        bytes = bigarrays.grow(bytes, offset);, +    }, +, +++ b/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java, +import org.apache.lucene.util.BytesRef;, +import org.elasticsearch.common.util.BigArrays;, +import org.elasticsearch.common.util.ByteArray;, + * A @link {@link StreamOutput} that uses{@link BigArrays} to acquire pages of, + * bytes, which avoids frequent reallocation & copying of the internal data., +    /**, +     * Factory/manager for our ByteArray, +     */, +    private final BigArrays bigarrays;, +     * The internal list of pages., +    private ByteArray bytes;, +    private int count;, +    /**, +     * Create a nonrecycling {@link BytesStreamOutput} with 1 initial page acquired., +     */, +        this(BigArrays.PAGE_SIZE_IN_BYTES);, +    /**, +     * Create a nonrecycling {@link BytesStreamOutput} with enough initial pages acquired, +     * to satisfy the capacity given by {@link expectedSize}., +     * , +     * @param expectedSize the expected maximum size of the stream in bytes., +     */]