[+++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java, +import org.elasticsearch.common.compress.lzf.LZFDecoder;, +        if (source == null) {, +            return null;, +        }, +        if (LZFDecoder.isCompressed(source)) {, +            try {, +                this.source = LZFDecoder.decode(source);, +            } catch (IOException e) {, +                throw new ElasticSearchParseException("failed to decompress source", e);, +            }, +        }, +        return Unicode.fromBytes(source());, +        byte[] source = source();, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java, +import org.elasticsearch.common.compress.lzf.LZFDecoder;, +        if (source == null) {, +            return null;, +        }, +        if (LZFDecoder.isCompressed(source)) {, +            try {, +                this.source = LZFDecoder.decode(source);, +            } catch (IOException e) {, +                throw new ElasticSearchParseException("failed to decompress source", e);, +            }, +        }, +        return Unicode.fromBytes(source());, +        byte[] source = source();, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java, +            source = documentMapper.sourceMapper().nativeValue(sourceField);, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java, +import org.elasticsearch.common.compress.lzf.LZFDecoder;, +        if (source == null) {, +            return null;, +        }, +        if (LZFDecoder.isCompressed(source)) {, +            try {, +                this.source = LZFDecoder.decode(source);, +            } catch (IOException e) {, +                throw new ElasticSearchParseException("failed to decompress source", e);, +            }, +        }, +        return Unicode.fromBytes(source());, +        byte[] source = source();, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java, +            source = documentMapper.sourceMapper().nativeValue(sourceField);, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/common/compress/lzf/LZFDecoder.java, +    public static boolean isCompressed(final byte[] buffer) {, +        return buffer.length >= 2 && buffer[0] == LZFChunk.BYTE_Z && buffer[1] == LZFChunk.BYTE_V;, +    }, +, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java, +import org.elasticsearch.common.compress.lzf.LZFDecoder;, +        if (source == null) {, +            return null;, +        }, +        if (LZFDecoder.isCompressed(source)) {, +            try {, +                this.source = LZFDecoder.decode(source);, +            } catch (IOException e) {, +                throw new ElasticSearchParseException("failed to decompress source", e);, +            }, +        }, +        return Unicode.fromBytes(source());, +        byte[] source = source();, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java, +            source = documentMapper.sourceMapper().nativeValue(sourceField);, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/common/compress/lzf/LZFDecoder.java, +    public static boolean isCompressed(final byte[] buffer) {, +        return buffer.length >= 2 && buffer[0] == LZFChunk.BYTE_Z && buffer[1] == LZFChunk.BYTE_V;, +    }, +, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/common/compress/lzf/LZFEncoder.java, +import org.elasticsearch.common.thread.ThreadLocals;, +, +    public static ThreadLocal<ThreadLocals.CleanableValue<ChunkEncoder>> cachedEncoder = new ThreadLocal<ThreadLocals.CleanableValue<ChunkEncoder>>() {, +        @Override protected ThreadLocals.CleanableValue<ChunkEncoder> initialValue() {, +            return new ThreadLocals.CleanableValue<ChunkEncoder>(new ChunkEncoder(LZFChunk.MAX_CHUNK_LEN));, +        }, +    };, +, +    public static byte[] encodeWithCache(byte[] data, int length) throws IOException {, +        int left = length;, +        ChunkEncoder enc = cachedEncoder.get().get();, +        int chunkLen = Math.min(LZFChunk.MAX_CHUNK_LEN, left);, +        LZFChunk first = enc.encodeChunk(data, 0, chunkLen);, +        left -= chunkLen;, +        // shortcut: if it all fit in, no need to coalesce:, +        if (left < 1) {, +            return first.getData();, +        }, +        // otherwise need to get other chunks:, +        int resultBytes = first.length();, +        int inputOffset = chunkLen;, +        LZFChunk last = first;, +, +        do {, +            chunkLen = Math.min(left, LZFChunk.MAX_CHUNK_LEN);, +            LZFChunk chunk = enc.encodeChunk(data, inputOffset, chunkLen);, +            inputOffset += chunkLen;]