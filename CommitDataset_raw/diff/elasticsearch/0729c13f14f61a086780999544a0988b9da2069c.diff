[+++ b/docs/en/index.asciidoc, +:docs-dir:              {docdir}/../../../../docs, +, +++ b/docs/en/index.asciidoc, +:docs-dir:              {docdir}/../../../../docs, +, +++ b/docs/en/installing-xes.asciidoc, +action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*, +For information, see, +{kibana-ref}/installing-xpack-kb.html[Installing {xpack} on {kib}] and, +{logstash-ref}/installing-xpack-log.html[Installing {xpack} on Logstash]., +++ b/docs/en/index.asciidoc, +:docs-dir:              {docdir}/../../../../docs, +, +++ b/docs/en/installing-xes.asciidoc, +action.auto_create_index: .security,.monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*, +For information, see, +{kibana-ref}/installing-xpack-kb.html[Installing {xpack} on {kib}] and, +{logstash-ref}/installing-xpack-log.html[Installing {xpack} on Logstash]., +++ b/docs/en/ml/getting-started-multi.asciidoc, +[[ml-gs-multi-jobs]], +=== Creating Multi-metric Jobs, +, +The multi-metric job wizard in {kib} provides a simple way to create more, +complex jobs with multiple detectors. For example, in the single metric job, you, +were tracking total requests versus time. You might also want to track other, +metrics like average response time or the maximum number of denied requests., +Instead of creating jobs for each of those metrics, you can combine them in a, +multi-metric job., +, +You can also use multi-metric jobs to split a single time series into multiple, +time series based on a categorical field. For example, you can split the data, +based on its hostnames, locations, or users. Each time series is modeled, +independently. By looking at temporal patterns on a per entity basis, you might, +spot things that might have otherwise been hidden in the lumped view., +, +Conceptually, you can think of this as running many independent single metric, +jobs. By bundling them together in a multi-metric job, however, you can see an, +overall score and shared influencers for all the metrics and all the entities in, +the job. Multi-metric jobs therefore scale better than having many independent, +single metric jobs and provide better results when you have influencers that are, +shared across the detectors., +, +The sample data for this tutorial contains information about the requests that, +are received by various applications and services in a system. Let's assume that, +you want to monitor the requests received and the response time.  In particular,, +you might want to track those metrics on a per service basis to see if any, +services have unusual patterns., +, +To create a multi-metric job in {kib}:, +, +. Open {kib} in your web browser and log in. If you are running {kib} locally,, +go to `http://localhost:5601/`., +, +. Click **Machine Learning** in the side navigation, then click **Create new job**. +, ++, +--, +[role="screenshot"], +image::images/ml-kibana.jpg[Job Management], +--, +, +. Click **Create multi metric job**. +, ++, +--, +[role="screenshot"], +image::images/ml-create-job2.jpg["Create a multi metric job"], +--, +, +. Click the `server-metrics` index. +, ++, +--, +[role="screenshot"], +image::images/ml-gs-index.jpg["Select an index"], +--, +, +. Configure the job by providing the following job settings: +, ++, +--, +[role="screenshot"], +image::images/ml-gs-multi-job.jpg["Create a new job from the server-metrics index"], +--, +, +.. For the **Fields**, select `high mean(response)` and `sum(total)`. This, +creates two detectors and specifies the analysis function and field that each, +detector uses. The first detector uses the high mean function to detect, +unusually high average values for the `response` field in each bucket. The, +second detector uses the sum function to detect when the sum of the `total`, +field is anomalous in each bucket. For more information about any of the, +analytical functions, see <<ml-functions>>., +, +.. For the **Bucket span**, enter `10m`. This value specifies the size of the, +interval that the analysis is aggregated into. As was the case in the single, +metric example, this value has a significant impact on the analysis. When you're, +creating jobs for your own data, you might need to experiment with different, +bucket spans depending on the frequency of the input data, the duration of, +typical anomalies, and the frequency at which alerting is required., +, +.. For the **Split Data**, select `service`. When you specify this, +option, the analysis is segmented such that you have completely independent, +baselines for each distinct value of this field.]