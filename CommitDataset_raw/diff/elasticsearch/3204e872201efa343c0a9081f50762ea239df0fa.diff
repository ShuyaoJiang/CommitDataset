[+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/build.gradle, +subprojects {, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/build.gradle, +subprojects {, +++ b/plugins/jvm-example/build.gradle, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/build.gradle, +subprojects {, +++ b/plugins/jvm-example/build.gradle, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/build.gradle, +subprojects {, +++ b/plugins/jvm-example/build.gradle, +++ /dev/null, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsPlugin.java, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/AntTask.groovy, +import org.apache.tools.ant.BuildException, +        final int outputLevel = logger.isDebugEnabled() ? Project.MSG_DEBUG : (logger.isInfoEnabled() ? Project.MSG_INFO : Project.MSG_WARN), +        } catch (BuildException e) {, +    protected abstract void runAnt(AntBuilder ant);, +++ /dev/null, +++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy, +++ b/docs/plugins/repository-hdfs.asciidoc, +Even if Hadoop is already installed on the Elasticsearch nodes, for security reasons, the required libraries need to be placed under the plugin folder. Note that in most cases, if the distro is compatible, one simply needs to configure the repository with the appropriate Hadoop configuration files (see below)., +    uri: "hdfs://<host>:<port>/"    \# required - HDFS address only, +    path: "some/path"               \# required - path within the file-system where data is stored/loaded, +++ b/plugins/build.gradle, +subprojects {]