[+++ b/docs/CHANGELOG.asciidoc, +A new analysis plugin called `analysis_nori` that exposes the Lucene Korean, +analysis module.  ({pull}30397[#30397]), +, +++ b/docs/CHANGELOG.asciidoc, +A new analysis plugin called `analysis_nori` that exposes the Lucene Korean, +analysis module.  ({pull}30397[#30397]), +, +++ b/docs/build.gradle, +  configFile 'userdict_ko.txt', +++ b/docs/CHANGELOG.asciidoc, +A new analysis plugin called `analysis_nori` that exposes the Lucene Korean, +analysis module.  ({pull}30397[#30397]), +, +++ b/docs/build.gradle, +  configFile 'userdict_ko.txt', +++ b/docs/plugins/analysis-nori.asciidoc, +[[analysis-nori]], +=== Korean (nori) Analysis Plugin, +, +The Korean (nori) Analysis plugin integrates Lucene nori analysis, +module into elasticsearch. It uses the https://bitbucket.org/eunjeon/mecab-ko-dic[mecab-ko-dic dictionary], +to perform morphological analysis of Korean texts., +, +:plugin_name: analysis-nori, +include::install_remove.asciidoc[], +, +[[analysis-nori-analyzer]], +==== `nori` analyzer, +, +The `nori` analyzer consists of the following tokenizer and token filters:, +, +* <<analysis-nori-tokenizer,`nori_tokenizer`>>, +* <<analysis-nori-speech,`nori_part_of_speech`>> token filter, +* <<analysis-nori-reading,`nori_readingform`>> token filter, +* {ref}/analysis-lowercase-tokenfilter.html[`lowercase`] token filter, +, +It supports the `decompound_mode` and `user_dictionary` settings from, +<<analysis-nori-tokenizer,`nori_tokenizer`>> and the `stoptags` setting from, +<<analysis-nori-speech,`nori_part_of_speech`>>., +, +[[analysis-nori-tokenizer]], +==== `nori_tokenizer`, +, +The `nori_tokenizer` accepts the following settings:, +, +`decompound_mode`::, ++, +--, +, +The decompound mode determines how the tokenizer handles compound tokens., +It can be set to:, +, +`none`::, +, +    No decomposition for compounds. Example output:, +, +    가거도항, +    가곡역, +, +`discard`::, +, +    Decomposes compounds and discards the original form (*default*). Example output:, +, +    가곡역 => 가곡, 역, +, +`mixed`::, +, +    Decomposes compounds and keeps the original form. Example output:, +, +    가곡역 => 가곡역, 가곡, 역, +--, +, +`user_dictionary`::, ++, +--, +The Nori tokenizer uses the https://bitbucket.org/eunjeon/mecab-ko-dic[mecab-ko-dic dictionary] by default., +A `user_dictionary` with custom nouns (`NNG`) may be appended to the default dictionary., +The dictionary should have the following format:, +, +[source,txt], +-----------------------, +<token> [<token 1> ... <token n>], +-----------------------, +, +The first token is mandatory and represents the custom noun that should be added in, +the dictionary. For compound nouns the custom segmentation can be provided, +after the first token (`[<token 1> ... <token n>]`). The segmentation of the, +custom compound nouns is controlled by the `decompound_mode` setting., +--, +, +As a demonstration of how the user dictionary can be used, save the following, +dictionary to `$ES_HOME/config/userdict_ko.txt`:, +, +[source,txt], +-----------------------, +c++                 <1>, +C샤프, +세종, +세종시 세종 시        <2>]