[+++ b/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc, +`adjust_offsets`::, +    By default, the filter tries to output subtokens with adjusted offsets, +    to reflect their actual position in the token stream.  However, when, +    used in combination with other filters that alter the length or starting, +    position of tokens without changing their offsets, +    (e.g. <<analysis-trim-tokenfilter,`trim`>>) this can cause tokens with, +    illegal offsets to be emitted.  Setting `adjust_offsets` to false will, +    stop `word_delimiter_graph` from adjusting these internal offsets., +, +++ b/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc, +`adjust_offsets`::, +    By default, the filter tries to output subtokens with adjusted offsets, +    to reflect their actual position in the token stream.  However, when, +    used in combination with other filters that alter the length or starting, +    position of tokens without changing their offsets, +    (e.g. <<analysis-trim-tokenfilter,`trim`>>) this can cause tokens with, +    illegal offsets to be emitted.  Setting `adjust_offsets` to false will, +    stop `word_delimiter_graph` from adjusting these internal offsets., +, +++ b/modules/analysis-common/src/main/java/org/elasticsearch/analysis/common/WordDelimiterGraphTokenFilterFactory.java, +    private final boolean adjustOffsets;, +        this.adjustOffsets = settings.getAsBoolean("adjust_offsets", true);, +        return new WordDelimiterGraphFilter(tokenStream, adjustOffsets, charTypeTable, flags, protoWords);, +++ b/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc, +`adjust_offsets`::, +    By default, the filter tries to output subtokens with adjusted offsets, +    to reflect their actual position in the token stream.  However, when, +    used in combination with other filters that alter the length or starting, +    position of tokens without changing their offsets, +    (e.g. <<analysis-trim-tokenfilter,`trim`>>) this can cause tokens with, +    illegal offsets to be emitted.  Setting `adjust_offsets` to false will, +    stop `word_delimiter_graph` from adjusting these internal offsets., +, +++ b/modules/analysis-common/src/main/java/org/elasticsearch/analysis/common/WordDelimiterGraphTokenFilterFactory.java, +    private final boolean adjustOffsets;, +        this.adjustOffsets = settings.getAsBoolean("adjust_offsets", true);, +        return new WordDelimiterGraphFilter(tokenStream, adjustOffsets, charTypeTable, flags, protoWords);, +++ b/modules/analysis-common/src/test/java/org/elasticsearch/analysis/common/WordDelimiterGraphTokenFilterFactoryTests.java, +        int[] expectedStartOffsets = new int[]{0, 0, 5};, +        int[] expectedEndOffsets = new int[]{9, 5, 9};, +        assertTokenStreamContents(tokenFilter.create(tokenizer), expected, expectedStartOffsets, expectedEndOffsets, null,, +            expectedIncr, expectedPosLen, null);, +    }, +, +    public void testAdjustingOffsets() throws IOException {, +        ESTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(, +            Settings.builder(), +                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()), +                .put("index.analysis.filter.my_word_delimiter.type", type), +                .put("index.analysis.filter.my_word_delimiter.catenate_words", "true"), +                .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true"), +                .put("index.analysis.filter.my_word_delimiter.adjust_offsets", "false"), +                .build(),, +            new CommonAnalysisPlugin());, +        TokenFilterFactory tokenFilter = analysis.tokenFilter.get("my_word_delimiter");, +        String source = "PowerShot";, +        int[] expectedIncr = new int[]{1, 0, 1};, +        int[] expectedPosLen = new int[]{2, 1, 1};, +        int[] expectedStartOffsets = new int[]{0, 0, 0};, +        int[] expectedEndOffsets = new int[]{9, 9, 9};, +        String[] expected = new String[]{"PowerShot", "Power", "Shot" };, +        Tokenizer tokenizer = new WhitespaceTokenizer();, +        tokenizer.setReader(new StringReader(source));, +        assertTokenStreamContents(tokenFilter.create(tokenizer), expected, expectedStartOffsets, expectedEndOffsets, null,, +++ b/docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc, +`adjust_offsets`::, +    By default, the filter tries to output subtokens with adjusted offsets, +    to reflect their actual position in the token stream.  However, when, +    used in combination with other filters that alter the length or starting, +    position of tokens without changing their offsets, +    (e.g. <<analysis-trim-tokenfilter,`trim`>>) this can cause tokens with, +    illegal offsets to be emitted.  Setting `adjust_offsets` to false will, +    stop `word_delimiter_graph` from adjusting these internal offsets., +, +++ b/modules/analysis-common/src/main/java/org/elasticsearch/analysis/common/WordDelimiterGraphTokenFilterFactory.java, +    private final boolean adjustOffsets;, +        this.adjustOffsets = settings.getAsBoolean("adjust_offsets", true);, +        return new WordDelimiterGraphFilter(tokenStream, adjustOffsets, charTypeTable, flags, protoWords);, +++ b/modules/analysis-common/src/test/java/org/elasticsearch/analysis/common/WordDelimiterGraphTokenFilterFactoryTests.java, +        int[] expectedStartOffsets = new int[]{0, 0, 5};, +        int[] expectedEndOffsets = new int[]{9, 5, 9};, +        assertTokenStreamContents(tokenFilter.create(tokenizer), expected, expectedStartOffsets, expectedEndOffsets, null,, +            expectedIncr, expectedPosLen, null);, +    }, +, +    public void testAdjustingOffsets() throws IOException {, +        ESTestCase.TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(, +            Settings.builder(), +                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()), +                .put("index.analysis.filter.my_word_delimiter.type", type), +                .put("index.analysis.filter.my_word_delimiter.catenate_words", "true"), +                .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true"), +                .put("index.analysis.filter.my_word_delimiter.adjust_offsets", "false"), +                .build(),, +            new CommonAnalysisPlugin());, +        TokenFilterFactory tokenFilter = analysis.tokenFilter.get("my_word_delimiter");, +        String source = "PowerShot";, +        int[] expectedIncr = new int[]{1, 0, 1};, +        int[] expectedPosLen = new int[]{2, 1, 1};]