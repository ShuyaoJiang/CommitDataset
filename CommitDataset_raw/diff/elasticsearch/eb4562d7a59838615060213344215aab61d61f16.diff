[+++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobContainer.java, +import org.apache.hadoop.fs.FSDataInputStream;, +import org.apache.lucene.util.IOUtils;, +import java.io.BufferedInputStream;, +import java.io.FilterInputStream;, +import java.security.AccessController;, +import java.security.PrivilegedActionException;, +import java.security.PrivilegedExceptionAction;, +            return store.execute(fileContext -> fileContext.util().exists(new Path(path, blobName)));, +        store.execute(fileContext -> fileContext.delete(new Path(path, blobName), true));, +        store.execute((Operation<Void>) fileContext -> {, +        // FSDataInputStream can open connections on read() or skip() so we wrap in, +        // HDFSPrivilegedInputSteam which will ensure that underlying methods will, +        // be called with the proper privileges., +        return store.execute(fileContext -> new HDFSPrivilegedInputSteam(fileContext.open(new Path(path, blobName), bufferSize)));, +        store.execute((Operation<Void>) fileContext -> {, +        FileStatus[] files = store.execute(fileContext -> (fileContext.util().listStatus(path,, +            path -> prefix == null || path.getName().startsWith(prefix))));, +, +    /**, +     * Exists to wrap underlying InputStream methods that might make socket connections in, +     * doPrivileged blocks. This is due to the way that hdfs client libraries might open, +     * socket connections when you are reading from an InputStream., +     */, +    private static class HDFSPrivilegedInputSteam extends FilterInputStream {, +, +        HDFSPrivilegedInputSteam(InputStream in) {, +            super(in);, +        }, +, +        public int read() throws IOException {, +            return doPrivilegedOrThrow(in::read);, +        }, +, +        public int read(byte b[]) throws IOException {, +            return doPrivilegedOrThrow(() -> in.read(b));, +        }, +, +        public int read(byte b[], int off, int len) throws IOException {, +            return doPrivilegedOrThrow(() -> in.read(b, off, len));, +        }, +, +        public long skip(long n) throws IOException {, +            return doPrivilegedOrThrow(() -> in.skip(n));, +        }, +, +        public int available() throws IOException {, +            return doPrivilegedOrThrow(() -> in.available());, +        }, +, +        public synchronized void reset() throws IOException {, +            doPrivilegedOrThrow(() -> {, +                in.reset();, +                return null;, +            });, +        }, +, +        private static  <T> T doPrivilegedOrThrow(PrivilegedExceptionAction<T> action) throws IOException {, +            try {, +                return AccessController.doPrivileged(action);, +            } catch (PrivilegedActionException e) {, +                throw (IOException) e.getCause();, +            }, +        }, +    }, +++ b/plugins/repository-hdfs/src/main/java/org/elasticsearch/repositories/hdfs/HdfsBlobContainer.java, +import org.apache.hadoop.fs.FSDataInputStream;, +import org.apache.lucene.util.IOUtils;, +import java.io.BufferedInputStream;, +import java.io.FilterInputStream;, +import java.security.AccessController;, +import java.security.PrivilegedActionException;, +import java.security.PrivilegedExceptionAction;, +            return store.execute(fileContext -> fileContext.util().exists(new Path(path, blobName)));, +        store.execute(fileContext -> fileContext.delete(new Path(path, blobName), true));, +        store.execute((Operation<Void>) fileContext -> {, +        // FSDataInputStream can open connections on read() or skip() so we wrap in, +        // HDFSPrivilegedInputSteam which will ensure that underlying methods will, +        // be called with the proper privileges., +        return store.execute(fileContext -> new HDFSPrivilegedInputSteam(fileContext.open(new Path(path, blobName), bufferSize)));, +        store.execute((Operation<Void>) fileContext -> {, +        FileStatus[] files = store.execute(fileContext -> (fileContext.util().listStatus(path,, +            path -> prefix == null || path.getName().startsWith(prefix))));, +, +    /**, +     * Exists to wrap underlying InputStream methods that might make socket connections in, +     * doPrivileged blocks. This is due to the way that hdfs client libraries might open, +     * socket connections when you are reading from an InputStream., +     */, +    private static class HDFSPrivilegedInputSteam extends FilterInputStream {, +, +        HDFSPrivilegedInputSteam(InputStream in) {, +            super(in);, +        }, +, +        public int read() throws IOException {, +            return doPrivilegedOrThrow(in::read);, +        }, +, +        public int read(byte b[]) throws IOException {]