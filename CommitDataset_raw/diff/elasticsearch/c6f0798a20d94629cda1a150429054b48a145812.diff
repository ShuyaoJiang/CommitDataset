[+++ b/docs/plugins/integrations.asciidoc, +* https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer[Kafka Standalone Consumer]:, +  Easily Scalable & Extendable Kafka Standalone Consumer that reads messages from Kafka, then processes and indexes the messages in ElasticSearch, +++ b/docs/plugins/integrations.asciidoc, +* https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer[Kafka Standalone Consumer]:, +  Easily Scalable & Extendable Kafka Standalone Consumer that reads messages from Kafka, then processes and indexes the messages in ElasticSearch, +++ b/docs/reference/analysis/analyzers/snowball-analyzer.asciidoc, +http://snowball.tartarus.org[snowball.tartarus.org]., +++ b/docs/plugins/integrations.asciidoc, +* https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer[Kafka Standalone Consumer]:, +  Easily Scalable & Extendable Kafka Standalone Consumer that reads messages from Kafka, then processes and indexes the messages in ElasticSearch, +++ b/docs/reference/analysis/analyzers/snowball-analyzer.asciidoc, +http://snowball.tartarus.org[snowball.tartarus.org]., +++ b/docs/resiliency/index.asciidoc, +=== Use two phase commit for Cluster State publishing (STATUS: ONGOING, v3.0.0), +=== Wait on incoming joins before electing local node as master (STATUS: DONE, v2.0.0), +, +During master election each node pings in order to discover other nodes and validate the liveness of existing, +nodes. Based on this information the node either discovers an existing master or, if enough nodes are found, +(see https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election[`discovery.zen.minimum_master_nodes`]) a new master will be elected. Currently, the node that is, +elected as master will update the cluster state to indicate the result of the election. Other nodes will submit, +a join request to the newly elected master node. Instead of immediately processing the election result, the elected master, +node should wait for the incoming joins from other nodes, thus validating that the result of the election is properly applied. As soon as enough, +nodes have sent their joins request (based on the `minimum_master_nodes` settings) the cluster state is updated., +{GIT}12161[#12161], +, +[float], +=== Mapping changes should be applied synchronously (STATUS: DONE, v2.0.0), +, +When introducing new fields using dynamic mapping, it is possible that the same, +field can be added to different shards with different data types.  Each shard, +will operate with its local data type but, if the shard is relocated, the, +data type from the cluster state will be applied to the new shard, which, +can result in a corrupt shard.  To prevent this, new fields should not, +be added to a shard's mapping until confirmed by the master., +{GIT}8688[#8688] (STATUS: DONE), +, +[float], +=== Add per-segment and per-commit ID to help replication (STATUS: DONE, v2.0.0), +, +{JIRA}5895[LUCENE-5895] adds a unique ID for each segment and each commit point. File-based replication (as performed by snapshot/restore) can use this ID to know whether the segment/commit on the source and destination machines are the same.  Fixed in Lucene 5.0., +, +[float], +=== Write index metadata on data nodes where shards allocated (STATUS: DONE, v2.0.0), +, +Today, index metadata is written only on nodes that are master-eligible, not on, +data-only nodes.  This is not a problem when running with multiple master nodes,, +as recommended, as the loss of all but one master node is still recoverable., +However, users running with a single master node are at risk of losing, +their index metadata if the master fails.  Instead, this metadata should, +also be written on any node where a shard is allocated. {GIT}8823[#8823], {GIT}9952[#9952], +, +[float], +=== Better file distribution with multiple data paths (STATUS: DONE, v2.0.0), +, +Today, a node configured with multiple data paths distributes writes across, +all paths by writing one file to each path in turn.  This can mean that the, +failure of a single disk corrupts many shards at once.  Instead, by allocating, +an entire shard to a single data path, the extent of the damage can be limited, +to just the shards on that disk. {GIT}9498[#9498], +, +[float], +=== Lucene checksums phase 3 (STATUS: DONE, v2.0.0), +, +Almost all files in Elasticsearch now have checksums which are validated before use.  A few changes remain:, +, +* {GIT}7586[#7586] adds checksums for cluster and index state files. (STATUS: DONE, Fixed in v1.5.0), +* {GIT}9183[#9183] supports validating the checksums on all files when starting a node. (STATUS: DONE, Fixed in v2.0.0), +* {JIRA}5894[LUCENE-5894] lays the groundwork for extending more efficient checksum validation to all files during optimized bulk merges. (STATUS: DONE, Fixed in v2.0.0), +* {GIT}8403[#8403] to add validation of checksums on Lucene `segments_N` files. (STATUS: DONE, v2.0.0), +, +[float], +=== Report shard-level statuses on write operations (STATUS: DONE, v2.0.0), +, +Make write calls return the number of total/successful/missing shards in the same way that we do in search, which ensures transparency in the consistency of write operations. {GIT}7994[#7994]. (STATUS: DONE, v2.0.0), +, +[float], +=== Take filter cache key size into account (STATUS: DONE, v2.0.0), +, +Commonly used filters are cached in Elasticsearch. That cache is limited in size, +(10% of node's memory by default) and is being evicted based on a least recently, +used policy. The amount of memory used by the cache depends on two primary, +components - the values it stores and the keys associated with them. Calculating, +the memory footprint of the values is easy enough but the keys accounting is, +trickier to achieve as they are, by default, raw Lucene objects. This is largely, +not a problem as the keys are dominated by the values. However, recent, +optimizations in Lucene have changed the balance causing the filter cache to, +grow beyond it's size., +, +While we are working on a longer term solution ({GIT}9176[#9176]), we introduced, +a minimum weight of 1k for each cache entry. This puts an effective limit on the number of entries in the cache. See {GIT}8304[#8304] (STATUS: DONE, fixed in v1.4.0), +, +Note: this has been solved by the move to Lucene's query cache. See {GIT}10897[#10897], +, +[float]]