[+++ b/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java, +                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());, +++ b/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java, +                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());, +++ b/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java, +                .put("cloud.aws.test.write_failures", 0.1), +                .put("cloud.aws.test.read_failures", 0.1);, +++ b/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java, +                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());, +++ b/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java, +                .put("cloud.aws.test.write_failures", 0.1), +                .put("cloud.aws.test.read_failures", 0.1);, +++ b/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java, +import com.amazonaws.services.s3.model.*;, +import org.elasticsearch.common.logging.ESLogger;, +import org.elasticsearch.common.logging.Loggers;, +    protected final ESLogger logger = Loggers.getLogger(getClass());, +, +    private double readFailureRate = 0.0;, +        readFailureRate = componentSettings.getAsDouble("test.read_failures", 0.0);, +            logger.info("--> random write failure on putObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);, +    @Override, +    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {, +        if (shouldFail(request.getBucketName(), request.getKey(), writeFailureRate)) {, +            long length = request.getPartSize();, +            long partToRead = (long) (length * randomDouble());, +            byte[] buffer = new byte[1024];, +            for (long cur = 0; cur < partToRead; cur += buffer.length) {, +                try (InputStream input = request.getInputStream()){, +                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));, +                } catch (IOException ex) {, +                    throw new ElasticsearchException("cannot read input stream", ex);, +                }, +            }, +            logger.info("--> random write failure on uploadPart method: throwing an exception for [bucket={}, key={}]", request.getBucketName(), request.getKey());, +            AmazonS3Exception ex = new AmazonS3Exception("Random S3 write exception");, +            ex.setStatusCode(400);, +            ex.setErrorCode("RequestTimeout");, +            throw ex;, +        } else {, +            return super.uploadPart(request);, +        }, +    }, +, +    @Override, +    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {, +        if (shouldFail(bucketName, key, readFailureRate)) {, +            logger.info("--> random read failure on getObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);, +            AmazonS3Exception ex = new AmazonS3Exception("Random S3 read exception");, +            ex.setStatusCode(404);, +            throw ex;, +        } else {, +            return super.getObject(bucketName, key);, +        }, +    }, +, +++ b/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java, +                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());, +++ b/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java, +                .put("cloud.aws.test.write_failures", 0.1), +                .put("cloud.aws.test.read_failures", 0.1);, +++ b/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java, +import com.amazonaws.services.s3.model.*;, +import org.elasticsearch.common.logging.ESLogger;, +import org.elasticsearch.common.logging.Loggers;, +    protected final ESLogger logger = Loggers.getLogger(getClass());, +, +    private double readFailureRate = 0.0;, +        readFailureRate = componentSettings.getAsDouble("test.read_failures", 0.0);, +            logger.info("--> random write failure on putObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);, +    @Override, +    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {, +        if (shouldFail(request.getBucketName(), request.getKey(), writeFailureRate)) {, +            long length = request.getPartSize();, +            long partToRead = (long) (length * randomDouble());, +            byte[] buffer = new byte[1024];, +            for (long cur = 0; cur < partToRead; cur += buffer.length) {, +                try (InputStream input = request.getInputStream()){, +                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));, +                } catch (IOException ex) {, +                    throw new ElasticsearchException("cannot read input stream", ex);, +                }, +            }, +            logger.info("--> random write failure on uploadPart method: throwing an exception for [bucket={}, key={}]", request.getBucketName(), request.getKey());, +            AmazonS3Exception ex = new AmazonS3Exception("Random S3 write exception");, +            ex.setStatusCode(400);, +            ex.setErrorCode("RequestTimeout");, +            throw ex;, +        } else {, +            return super.uploadPart(request);, +        }, +    }, +, +    @Override, +    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {, +        if (shouldFail(bucketName, key, readFailureRate)) {, +            logger.info("--> random read failure on getObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);, +            AmazonS3Exception ex = new AmazonS3Exception("Random S3 read exception");, +            ex.setStatusCode(404);, +            throw ex;]