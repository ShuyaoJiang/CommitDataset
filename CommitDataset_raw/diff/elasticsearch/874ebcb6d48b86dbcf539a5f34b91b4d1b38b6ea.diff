[+++ b/docs/build.gradle, +/* List of files that have snippets that require a gold or platinum licence , +and therefore cannot be tested yet... */, +buildRestTests.expectedUnconvertedCandidates = [, +        'reference/ml/transforms.asciidoc',, +], +, +++ b/docs/build.gradle, +/* List of files that have snippets that require a gold or platinum licence , +and therefore cannot be tested yet... */, +buildRestTests.expectedUnconvertedCandidates = [, +        'reference/ml/transforms.asciidoc',, +], +, +++ b/docs/reference/ml/aggregations.asciidoc, +[role="xpack"], +[[ml-configuring-aggregation]], +=== Aggregating data for faster performance, +, +By default, {dfeeds} fetch data from {es} using search and scroll requests., +It can be significantly more efficient, however, to aggregate data in {es}, +and to configure your jobs to analyze aggregated data., +, +One of the benefits of aggregating data this way is that {es} automatically, +distributes these calculations across your cluster. You can then feed this, +aggregated data into {xpackml} instead of raw results, which, +reduces the volume of data that must be considered while detecting anomalies., +, +There are some limitations to using aggregations in {dfeeds}, however., +Your aggregation must include a `date_histogram` aggregation, which in turn must, +contain a `max` aggregation on the time field. This requirement ensures that the, +aggregated data is a time series and the timestamp of each bucket is the time, +of the last record in the bucket. If you use a terms aggregation and the, +cardinality of a term is high, then the aggregation might not be effective and, +you might want to just use the default search and scroll behavior., +, +When you create or update a job, you can include the names of aggregations, for, +example:, +, +[source,js], +----------------------------------, +PUT _xpack/ml/anomaly_detectors/farequote, +{, +  "analysis_config": {, +    "bucket_span": "60m",, +    "detectors": [{, +      "function": "mean",, +      "field_name": "responsetime",, +      "by_field_name": "airline", +    }],, +    "summary_count_field_name": "doc_count", +  },, +  "data_description": {, +    "time_field":"time", +  }, +}, +----------------------------------, +// CONSOLE, +// TEST[skip:setup:farequote_data], +, +In this example, the `airline`, `responsetime`, and `time` fields are, +aggregations., +, +NOTE: When the `summary_count_field_name` property is set to a non-null value,, +the job expects to receive aggregated input. The property must be set to the, +name of the field that contains the count of raw data points that have been, +aggregated. It applies to all detectors in the job., +, +The aggregations are defined in the {dfeed} as follows:, +, +[source,js], +----------------------------------, +PUT _xpack/ml/datafeeds/datafeed-farequote, +{, +  "job_id":"farequote",, +  "indices": ["farequote"],, +  "types": ["response"],, +  "aggregations": {, +    "buckets": {, +      "date_histogram": {, +        "field": "time",, +        "interval": "360s",, +        "time_zone": "UTC", +      },, +      "aggregations": {, +        "time": {, +          "max": {"field": "time"}, +        },, +        "airline": {, +          "terms": {, +            "field": "airline",, +            "size": 100, +          },, +          "aggregations": {, +            "responsetime": {, +              "avg": {, +                "field": "responsetime", +              }, +            }, +          }]