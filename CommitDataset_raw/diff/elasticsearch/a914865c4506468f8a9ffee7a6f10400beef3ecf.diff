[+++ b/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java, +                    // remove from the current state all the shards that are primary and started somewhere, we won't need them anymore, +                    // and if they are still here, we will add them in the next phase, +, +                    // Also note, this works well when closing an index, since a closed index will have no routing shards entries, +                    // so they won't get removed (we want to keep the fact that those shards are allocated on this node if needed), +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java, +                    // remove from the current state all the shards that are primary and started somewhere, we won't need them anymore, +                    // and if they are still here, we will add them in the next phase, +, +                    // Also note, this works well when closing an index, since a closed index will have no routing shards entries, +                    // so they won't get removed (we want to keep the fact that those shards are allocated on this node if needed), +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java, +            IndexMetaData indexMetaData = event.state().metaData().index(index);, +            if (indexMetaData != null) {, +                        if (indexMetaData.state() == IndexMetaData.State.CLOSE) {, +                                logger.debug("[{}][{}] removing shard (index is closed)", index, existingShardId);, +                            }, +                            indexService.removeShard(existingShardId);, +                        } else {, +                            if (logger.isDebugEnabled()) {, +                                logger.debug("[{}][{}] cleaning shard locally (not allocated)", index, existingShardId);, +    }, +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java, +                    // remove from the current state all the shards that are primary and started somewhere, we won't need them anymore, +                    // and if they are still here, we will add them in the next phase, +, +                    // Also note, this works well when closing an index, since a closed index will have no routing shards entries, +                    // so they won't get removed (we want to keep the fact that those shards are allocated on this node if needed), +++ b/modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java, +            IndexMetaData indexMetaData = event.state().metaData().index(index);, +            if (indexMetaData != null) {, +                        if (indexMetaData.state() == IndexMetaData.State.CLOSE) {, +                                logger.debug("[{}][{}] removing shard (index is closed)", index, existingShardId);, +                            }, +                            indexService.removeShard(existingShardId);, +                        } else {, +                            if (logger.isDebugEnabled()) {, +                                logger.debug("[{}][{}] cleaning shard locally (not allocated)", index, existingShardId);, +    }, +++ b/modules/test/integration/src/test/java/org/elasticsearch/test/integration/gateway/local/LocalGatewayIndexStateTests.java, +        logger.info("--> creating another index (test2) by indexing into it");, +        client("node1").prepareIndex("test2", "type1", "1").setSource("field1", "value1").execute().actionGet();, +        logger.info("--> verifying that the state is green");, +        health = client("node1").admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForNodes("2").execute().actionGet();, +        assertThat(health.timedOut(), equalTo(false));, +        assertThat(health.status(), equalTo(ClusterHealthStatus.GREEN));, +, +        logger.info("--> opening the first index again...");, +        client("node1").admin().indices().prepareOpen("test").execute().actionGet();, +, +        logger.info("--> verifying that the state is green");, +        health = client("node1").admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForNodes("2").execute().actionGet();, +        assertThat(health.timedOut(), equalTo(false));, +        assertThat(health.status(), equalTo(ClusterHealthStatus.GREEN));, +, +        stateResponse = client("node1").admin().cluster().prepareState().execute().actionGet();, +        assertThat(stateResponse.state().metaData().index("test").state(), equalTo(IndexMetaData.State.OPEN));, +        assertThat(stateResponse.state().routingTable().index("test").shards().size(), equalTo(2));, +        assertThat(stateResponse.state().routingTable().index("test").shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));, +, +        logger.info("--> trying to get the indexed document on the first index");, +        GetResponse getResponse = client("node1").prepareGet("test", "type1", "1").execute().actionGet();, +        assertThat(getResponse.exists(), equalTo(true));, +, +        logger.info("--> closing test index...");, +        client("node1").admin().indices().prepareClose("test").execute().actionGet();, +        stateResponse = client("node1").admin().cluster().prepareState().execute().actionGet();, +        assertThat(stateResponse.state().metaData().index("test").state(), equalTo(IndexMetaData.State.CLOSE));, +        assertThat(stateResponse.state().routingTable().index("test"), nullValue());, +, +        logger.info("--> waiting for two nodes and green status");, +        health = client("node1").admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForNodes("2").execute().actionGet();, +        getResponse = client("node1").prepareGet("test", "type1", "1").execute().actionGet();]