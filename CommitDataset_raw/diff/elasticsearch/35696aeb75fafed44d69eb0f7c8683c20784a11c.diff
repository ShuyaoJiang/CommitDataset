[+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java, +    public SignificantLongTerms(long subsetSize, long supersetSize, String name, ValueFormatter valueFormatter,, +            int requiredSize, long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +, +            buckets.add(new Bucket(subsetDf, subsetSize, supersetDf,supersetSize, term, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java, +    public SignificantLongTerms(long subsetSize, long supersetSize, String name, ValueFormatter valueFormatter,, +            int requiredSize, long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +, +            buckets.add(new Bucket(subsetDf, subsetSize, supersetDf,supersetSize, term, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java, +, +    private final SignificantTermsAggregatorFactory termsAggFactory;, +        for (long i = 0; i < bucketOrds.capacity(); i++) {, +            // During shard-local down-selection we use subset/superset stats that are for this shard only, +            // Back at the central reducer these properties will be updated with global stats, +        for (int i = ordered.size() - 1; i >= 0; i--) {, +        // We need to account for the significance of a miss in our global stats - provide corpus size as context, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java, +    public SignificantLongTerms(long subsetSize, long supersetSize, String name, ValueFormatter valueFormatter,, +            int requiredSize, long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +, +            buckets.add(new Bucket(subsetDf, subsetSize, supersetDf,supersetSize, term, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java, +, +    private final SignificantTermsAggregatorFactory termsAggFactory;, +        for (long i = 0; i < bucketOrds.capacity(); i++) {, +            // During shard-local down-selection we use subset/superset stats that are for this shard only, +            // Back at the central reducer these properties will be updated with global stats, +        for (int i = ordered.size() - 1; i >= 0; i--) {, +        // We need to account for the significance of a miss in our global stats - provide corpus size as context, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java, +    public SignificantStringTerms(long subsetSize, long supersetSize, String name, int requiredSize,, +            long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +            buckets.add(new Bucket(term, subsetDf, subsetSize, supersetDf, supersetSize, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java, +    public SignificantLongTerms(long subsetSize, long supersetSize, String name, ValueFormatter valueFormatter,, +            int requiredSize, long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +, +            buckets.add(new Bucket(subsetDf, subsetSize, supersetDf,supersetSize, term, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java, +, +    private final SignificantTermsAggregatorFactory termsAggFactory;, +        for (long i = 0; i < bucketOrds.capacity(); i++) {, +            // During shard-local down-selection we use subset/superset stats that are for this shard only, +            // Back at the central reducer these properties will be updated with global stats, +        for (int i = ordered.size() - 1; i >= 0; i--) {, +        // We need to account for the significance of a miss in our global stats - provide corpus size as context, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java, +    public SignificantStringTerms(long subsetSize, long supersetSize, String name, int requiredSize,, +            long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +            buckets.add(new Bucket(term, subsetDf, subsetSize, supersetDf, supersetSize, InternalAggregations.readAggregations(in)));, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java, +    protected final SignificantTermsAggregatorFactory termsAggFactory;, +            IncludeExclude includeExclude, AggregationContext aggregationContext, Aggregator parent,, +            SignificantTermsAggregatorFactory termsAggFactory) {, +, +        super(name, factories, valuesSource, estimatedBucketCount, null, requiredSize, shardSize,, +                minDocCount, includeExclude, aggregationContext, parent);, +        for (int i = ordered.size() - 1; i >= 0; i--) {, +        // We need to account for the significance of a miss in our global stats - provide corpus size as context, +                long esitmatedBucketCount, int requiredSize, int shardSize, long minDocCount, AggregationContext aggregationContext,, +                Aggregator parent, SignificantTermsAggregatorFactory termsAggFactory) {, +                if (bucketOrd < 0) { // unlikely condition on a low-cardinality field, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java, +, +         * <p/>, +        public static double getSampledTermSignificance(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize) {, +++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java, +    public SignificantLongTerms(long subsetSize, long supersetSize, String name, ValueFormatter valueFormatter,, +            int requiredSize, long minDocCount, Collection<InternalSignificantTerms.Bucket> buckets) {, +, +            buckets.add(new Bucket(subsetDf, subsetSize, supersetDf,supersetSize, term, InternalAggregations.readAggregations(in)));]