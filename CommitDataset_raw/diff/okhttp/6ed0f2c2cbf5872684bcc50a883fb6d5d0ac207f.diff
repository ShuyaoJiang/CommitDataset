[+++ b/okhttp/src/main/java/com/squareup/okhttp/internal/DiskLruCache.java, +  static final String JOURNAL_FILE_TEMP = "journal.tmp";, +  static final String JOURNAL_FILE_BACKUP = "journal.bkp";, +  private final File journalFileBackup;, +  private final LinkedHashMap<String, Entry> lruEntries =, +      new LinkedHashMap<String, Entry>(0, 0.75f, true);, +  final ThreadPoolExecutor executorService =, +      new ThreadPoolExecutor(0, 1, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());, +          return null; // Closed., +    this.journalFileTmp = new File(directory, JOURNAL_FILE_TEMP);, +    this.journalFileBackup = new File(directory, JOURNAL_FILE_BACKUP);, +    // If a bkp file exists, use it instead., +    File backupFile = new File(directory, JOURNAL_FILE_BACKUP);, +    if (backupFile.exists()) {, +      // If journal file also exists just delete backup file., +        backupFile.delete();, +        renameTo(backupFile, journalFile, false);, +    // Prefer to pick up where we left off., +        cache.journalWriter = new BufferedWriter(, +            new OutputStreamWriter(new FileOutputStream(cache.journalFile, true), Util.US_ASCII));, +    // Create a new empty cache., +    StrictLineReader reader = new StrictLineReader(new FileInputStream(journalFile), Util.US_ASCII);, +        throw new IOException("unexpected journal header: [" + magic + ", " + version + ", ", +            + valueCountString + ", " + blank + "]");, +      // This work was already done by calling lruEntries.get()., +    Writer writer = new BufferedWriter(, +        new OutputStreamWriter(new FileOutputStream(journalFileTmp), Util.US_ASCII));, +      renameTo(journalFile, journalFileBackup, true);, +    journalFileBackup.delete();, +    journalWriter = new BufferedWriter(, +        new OutputStreamWriter(new FileOutputStream(journalFile, true), Util.US_ASCII));, +  private static void renameTo(File from, File to, boolean deleteDestination) throws IOException {, +    // Open all streams eagerly to guarantee that we see a single published, +    // snapshot. If we opened streams lazily then the streams could come, +    // from different edits., +      // A file must have been deleted manually!, +    if (expectedSequenceNumber != ANY_SEQUENCE_NUMBER && (entry == null, +        || entry.sequenceNumber != expectedSequenceNumber)) {, +      return null; // Snapshot is stale., +      return null; // Another edit is in progress., +    // Flush the journal before creating files to prevent file leaks., +  /** Returns the directory where this cache stores its data. */, +    // If this edit is creating the entry for the first time, every index must have a value., +    final int redundantOpCompactThreshold = 2000;, +    return redundantOpCount >= redundantOpCompactThreshold //, +        && redundantOpCount >= lruEntries.size();, +  /** Returns true if this cache has been closed. */, +  /** Force buffered operations to the filesystem. */, +  /** Closes this cache. Stored values will remain on the filesystem. */, +      return; // Already closed., +      throw new IllegalArgumentException("keys must match regex [a-z0-9_-]{1,64}: \"" + key + "\"");, +  /** A snapshot of the values for an entry. */, +    /** Returns the unbuffered stream with the value for {@code index}. */, +    /** Returns the string value for {@code index}. */, +    /** Returns the byte length of the value for {@code index}. */, +  /** Edits the values for an entry. */, +    /** Sets the value at {@code index} to {@code value}. */, +        remove(entry.key); // The previous entry is stale., +    /** Set lengths using decimal numbers like "10123". */, +++ b/okhttp/src/main/java/com/squareup/okhttp/internal/DiskLruCache.java, +  static final String JOURNAL_FILE_TEMP = "journal.tmp";, +  static final String JOURNAL_FILE_BACKUP = "journal.bkp";, +  private final File journalFileBackup;, +  private final LinkedHashMap<String, Entry> lruEntries =, +      new LinkedHashMap<String, Entry>(0, 0.75f, true);, +  final ThreadPoolExecutor executorService =, +      new ThreadPoolExecutor(0, 1, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());, +          return null; // Closed., +    this.journalFileTmp = new File(directory, JOURNAL_FILE_TEMP);, +    this.journalFileBackup = new File(directory, JOURNAL_FILE_BACKUP);, +    // If a bkp file exists, use it instead., +    File backupFile = new File(directory, JOURNAL_FILE_BACKUP);, +    if (backupFile.exists()) {, +      // If journal file also exists just delete backup file., +        backupFile.delete();, +        renameTo(backupFile, journalFile, false);, +    // Prefer to pick up where we left off., +        cache.journalWriter = new BufferedWriter(, +            new OutputStreamWriter(new FileOutputStream(cache.journalFile, true), Util.US_ASCII));, +    // Create a new empty cache., +    StrictLineReader reader = new StrictLineReader(new FileInputStream(journalFile), Util.US_ASCII);, +        throw new IOException("unexpected journal header: [" + magic + ", " + version + ", ", +            + valueCountString + ", " + blank + "]");, +      // This work was already done by calling lruEntries.get()., +    Writer writer = new BufferedWriter(, +        new OutputStreamWriter(new FileOutputStream(journalFileTmp), Util.US_ASCII));, +      renameTo(journalFile, journalFileBackup, true);, +    journalFileBackup.delete();, +    journalWriter = new BufferedWriter(, +        new OutputStreamWriter(new FileOutputStream(journalFile, true), Util.US_ASCII));, +  private static void renameTo(File from, File to, boolean deleteDestination) throws IOException {, +    // Open all streams eagerly to guarantee that we see a single published, +    // snapshot. If we opened streams lazily then the streams could come, +    // from different edits., +      // A file must have been deleted manually!, +    if (expectedSequenceNumber != ANY_SEQUENCE_NUMBER && (entry == null, +        || entry.sequenceNumber != expectedSequenceNumber)) {, +      return null; // Snapshot is stale., +      return null; // Another edit is in progress., +    // Flush the journal before creating files to prevent file leaks.]