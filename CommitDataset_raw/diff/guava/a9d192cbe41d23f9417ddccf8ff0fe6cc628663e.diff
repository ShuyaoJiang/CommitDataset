[+++ b/guava/src/com/google/common/collect/ComputingConcurrentHashMap.java, +    return segmentFor(hash).compute(key, hash);, +    V compute(K key, int hash) {, +                  if (!e.getValueReference().isComputingReference()) {, +                    // clobber invalid entries, +                    unsetLiveEntry(e, hash);, +              if (e == null || isUnset(e)) {, +                // Create a new entry., +                ComputingConcurrentHashMap<K, V> computingMap =, +                    (ComputingConcurrentHashMap<K, V>) map;, +                computingValueReference = new ComputingValueReference<K, V>(computingMap);, +                  e = computingMap.newEntry(key, hash, first);, +                // Synchronizes on the entry to allow failing fast when a, +                // recursive computation is detected. This is not fool-proof, +                // since the entry may be copied when the segment is written to., +    final ComputingConcurrentHashMap<K, V> map;, +    public ComputingValueReference(ComputingConcurrentHashMap<K, V> map) {, +      this.map = map;, +        value = map.computingFunction.apply(key);, +      if (value != null) {, +        // Call setValueReference first to avoid put clearing us., +        // TODO(user): recordMiss, +        // TODO(user): recordCompute, +        // putIfAbsent, +        map.segmentFor(hash).put(key, hash, value, true);, +      }, +, +++ b/guava/src/com/google/common/collect/ComputingConcurrentHashMap.java, +    return segmentFor(hash).compute(key, hash);, +    V compute(K key, int hash) {, +                  if (!e.getValueReference().isComputingReference()) {, +                    // clobber invalid entries, +                    unsetLiveEntry(e, hash);, +              if (e == null || isUnset(e)) {, +                // Create a new entry., +                ComputingConcurrentHashMap<K, V> computingMap =, +                    (ComputingConcurrentHashMap<K, V>) map;, +                computingValueReference = new ComputingValueReference<K, V>(computingMap);, +                  e = computingMap.newEntry(key, hash, first);, +                // Synchronizes on the entry to allow failing fast when a, +                // recursive computation is detected. This is not fool-proof, +                // since the entry may be copied when the segment is written to., +    final ComputingConcurrentHashMap<K, V> map;, +    public ComputingValueReference(ComputingConcurrentHashMap<K, V> map) {, +      this.map = map;, +        value = map.computingFunction.apply(key);, +      if (value != null) {, +        // Call setValueReference first to avoid put clearing us., +        // TODO(user): recordMiss, +        // TODO(user): recordCompute, +        // putIfAbsent, +        map.segmentFor(hash).put(key, hash, value, true);, +      }, +, +++ b/guava/src/com/google/common/collect/CustomConcurrentHashMap.java, +import java.util.LinkedList;, +    return cleanupExecutor == MapMaker.DEFAULT_CLEANUP_EXECUTOR;, +   * - Unset: marked as unset, awaiting cleanup or reuse, +    Segment<K, V> segment = segmentFor(hash);, +    segment.unsetValue(entry.getKey(), hash, valueReference);, +    if (!segment.isHeldByCurrentThread()) { // don't cleanup inside of put, +      segment.postWriteCleanup();, +    }, +    segmentFor(hash).unsetKey(entry, hash);, +     * The number of live elements in this segment's region. This does not include unset elements, +     * which are awaiting cleanup., +     * The cleanup queue is used to record entries which have been unset and need to be removed, +     * from the map. It is drained by the cleanup executor., +     */, +    // TODO(user): switch to ArrayDeque if we ever require JDK 1.6, +    @GuardedBy("Segment.this"), +    final Queue<ReferenceEntry<K, V>> cleanupQueue = new LinkedList<ReferenceEntry<K, V>>();, +, +    /**, +        if (!unsetEntry(e, e.getHash())) {, +        if (!unsetEntry(e, e.getHash())) {, +        for (ReferenceEntry<K, V> e = getFirst(hash); e != null; e = e.getNext()) {, +            V entryValue = e.getValueReference().get();, +              unsetLiveEntry(e, hash);, +        for (ReferenceEntry<K, V> e = getFirst(hash); e != null; e = e.getNext()) {, +            V entryValue = e.getValueReference().get();, +              unsetLiveEntry(e, hash);, +              // Value could be partially-collected, unset, or computing., +              // In the first case, the value must be reclaimed. In the latter, +              // two cases en entry must be evicted. This entry could be both, +              // partially-collected and next on the eviction list, which is why, +              // notifyValueReclaimed must be called prior to evictEntries., +              valueReference.notifyValueReclaimed();, +            }, +            // else clobber, don't adjust count, +, +                unsetLiveEntry(e, e.getHash()); // decrements count, +            V entryValue = e.getValueReference().get();, +              unsetLiveEntry(e, hash);, +              ReferenceEntry<K, V> newFirst = removeFromChain(first, e); // could decrement count, +            V entryValue = e.getValueReference().get();, +              unsetLiveEntry(e, hash);, +              ReferenceEntry<K, V> newFirst = removeFromChain(first, e); // could decrement count, +     * all preceding ones need to be cloned., +          unsetLiveEntry(e, e.getHash()); // decrements count]