[+++ b/src/com/google/common/collect/ComputingConcurrentHashMap.java, +        ReferenceEntry<K, V> entry = getLiveEntry(key, hash);, +        if (entry == null) { // entry is absent or invalid, +            expireEntries();, +            int newCount = this.count + 1;, +            if (newCount > this.threshold) { // ensure capacity, +            // getFirst, but remember the index, +, +            for (ReferenceEntry<K, V> e = first; e != null; e = e.getNext()) {, +              K entryKey = e.getKey();, +              if (e.getHash() == hash && entryKey != null, +                  && keyEquivalence.equivalent(key, entryKey)) {, +                entry = e;, +                break;, +              }, +            }, +            if (entry == null || isInvalid(entry)) {, +              // Create a new entry., +              computingValueReference = new ComputingValueReference();, +, +              if (evictEntries()) {, +                newCount = this.count + 1;, +                first = table.get(index);, +              }, +, +              if (entry == null) {, +              }, +              // recordWrite at computation start because count is incremented, +              recordWrite(entry);, +              entry.setValueReference(computingValueReference);, +            } else {, +              recordRead(entry);, +            scheduleCleanup();, +          if (computingValueReference != null) {, +                scheduleCleanup();, +                scheduleCleanup();, +        segment.scheduleCleanup();, +++ b/src/com/google/common/collect/ComputingConcurrentHashMap.java, +        ReferenceEntry<K, V> entry = getLiveEntry(key, hash);, +        if (entry == null) { // entry is absent or invalid, +            expireEntries();, +            int newCount = this.count + 1;, +            if (newCount > this.threshold) { // ensure capacity, +            // getFirst, but remember the index, +, +            for (ReferenceEntry<K, V> e = first; e != null; e = e.getNext()) {, +              K entryKey = e.getKey();, +              if (e.getHash() == hash && entryKey != null, +                  && keyEquivalence.equivalent(key, entryKey)) {, +                entry = e;, +                break;, +              }, +            }, +            if (entry == null || isInvalid(entry)) {, +              // Create a new entry., +              computingValueReference = new ComputingValueReference();, +, +              if (evictEntries()) {, +                newCount = this.count + 1;, +                first = table.get(index);, +              }, +, +              if (entry == null) {, +              }, +              // recordWrite at computation start because count is incremented, +              recordWrite(entry);, +              entry.setValueReference(computingValueReference);, +            } else {, +              recordRead(entry);, +            scheduleCleanup();, +          if (computingValueReference != null) {, +                scheduleCleanup();, +                scheduleCleanup();, +        segment.scheduleCleanup();, +++ b/src/com/google/common/collect/CustomConcurrentHashMap.java, +import java.util.concurrent.Executor;, +  /**, +   * Maximum number of entries to be cleaned up in a single cleanup run., +   * TODO(user): empirically optimize this, +   */, +  static final int CLEANUP_MAX = 16;, +, +  /** Performs map housekeeping operations. */, +  final Executor cleanupExecutor;, +, +        EntryFactory.getFactory(keyStrength, expires(), evictsBySize());, +    cleanupExecutor = builder.getCleanupExecutor();, +        && (!evictsBySize() || segmentCount * 2 <= maximumSize)) {, +    if (evictsBySize()) {, +    segment.scheduleCleanup();, +      if (!evictsBySize() && !expiresAfterAccess()) {, +        return;, +      }, +, +      if (!evictsBySize() && !expires()) {, +        return;, +      }, +, +      processPendingCleanup();, +     * Performs eviction if the segment is full. This should only be called]