[+++ b/guava/src/com/google/common/collect/ComputingConcurrentHashMap.java, +    return segmentFor(hash).compute(key, hash, computingFunction);, +    V compute(K key, int hash, Function<? super K, ? extends V> computingFunction) {, +              boolean createNewEntry = true;, +                  ValueReference<K, V> valueReference = e.getValueReference();, +                  if (valueReference.isComputingReference()) {, +                    createNewEntry = false;, +                  } else {, +                    // immediately reuse invalid entries, +                    removeLiveEntry(e, hash);, +              if (createNewEntry) {, +                computingValueReference = new ComputingValueReference<K, V>(computingFunction);, +                  e = map.newEntry(key, hash, first);, +, +                // Synchronizes on the entry to allow failing fast when a recursive computation is, +                // detected. This is not fool-proof since the entry may be copied when the segment, +                // is written to., +                if (value != null) {, +                  // TODO(user): recordMiss, +                  // TODO(user): recordCompute, +                  // putIfAbsent, +                  put(key, hash, value, true);, +                }, +    final Function<? super K, ? extends V> computingFunction;, +    public ComputingValueReference(Function<? super K, ? extends V> computingFunction) {, +      this.computingFunction = computingFunction;, +        value = computingFunction.apply(key);, +++ b/guava/src/com/google/common/collect/ComputingConcurrentHashMap.java, +    return segmentFor(hash).compute(key, hash, computingFunction);, +    V compute(K key, int hash, Function<? super K, ? extends V> computingFunction) {, +              boolean createNewEntry = true;, +                  ValueReference<K, V> valueReference = e.getValueReference();, +                  if (valueReference.isComputingReference()) {, +                    createNewEntry = false;, +                  } else {, +                    // immediately reuse invalid entries, +                    removeLiveEntry(e, hash);, +              if (createNewEntry) {, +                computingValueReference = new ComputingValueReference<K, V>(computingFunction);, +                  e = map.newEntry(key, hash, first);, +, +                // Synchronizes on the entry to allow failing fast when a recursive computation is, +                // detected. This is not fool-proof since the entry may be copied when the segment, +                // is written to., +                if (value != null) {, +                  // TODO(user): recordMiss, +                  // TODO(user): recordCompute, +                  // putIfAbsent, +                  put(key, hash, value, true);, +                }, +    final Function<? super K, ? extends V> computingFunction;, +    public ComputingValueReference(Function<? super K, ? extends V> computingFunction) {, +      this.computingFunction = computingFunction;, +        value = computingFunction.apply(key);, +++ b/guava/src/com/google/common/collect/CustomConcurrentHashMap.java, +, +    // schedule cleanup after construction is complete, +    if (cleanupExecutor != null) {, +      cleanupExecutor.execute(cleanupTask);, +  }, +, +  final Runnable cleanupTask = new Runnable() {, +    @Override, +    public void run() {, +      // TODO(user): should we sleep between runs? use a scheduled executor? have a dirty, +      // flag to indicate when cleanup is required? enqueue one cleanup task per queue so they, +      // can wait on the queues?, +      for (Segment<K, V> segment : segments) {, +        segment.runCleanup();, +      }, +      cleanupExecutor.execute(cleanupTask);, +    }, +  };, +    return cleanupExecutor == null;, +    segmentFor(hash).reclaimValue(entry.getKey(), hash, valueReference);, +    segmentFor(hash).reclaimKey(entry, hash);, +          postWriteCleanup();, +        if (!removeEntry(e, e.getHash())) {, +        if (!removeEntry(e, e.getHash())) {, +              setValue(e, value);, +              return entryValue;, +            } else {, +              // clobber existing entry, count remains unchanged, +        }, +                removeLiveEntry(e, e.getHash()); // decrements count, +    boolean replace(K key, int hash, V oldValue, V newValue) {, +      checkNotNull(oldValue);, +      checkNotNull(newValue);, +      lock();, +      try {, +        preWriteCleanup();, +, +        for (ReferenceEntry<K, V> e = getFirst(hash); e != null; e = e.getNext()) {, +          K entryKey = e.getKey();, +          if (e.getHash() == hash && entryKey != null, +              && map.keyEquivalence.equivalent(key, entryKey)) {, +            // If the value disappeared, this entry is partially collected,, +            // and we should pretend like it doesn't exist., +            V entryValue = e.getValueReference().get();, +            if (entryValue == null) {]